\subsubsection{Text preprocessing}
\label{sub:text_preprocessing}

In general, neural networks require numerical inputs, i.e., scalars or vectors.
For example, images can be processed by looking at pixel values representing
coloration, which can either be scalar (grey-scale image) or multi-dimensional
(RGB image).
Consequently, in order to include tweet texts as input to a neural network,
the respective texts need to be transformed into numbers.
This is usually achieved by obtaining vector representations for single words.
Such a mapping between words and vectors of real numbers is called a \textbf{word
embedding}.
This section is structured as follows.
First, an introduction to word vectors are given.
Afterwards, all five steps of the transformation process are be presented using a simple example.
In addition, intermediate results of the preprocessing steps for the collected
data sets are described.
The example used throughout this section comprises two fictional tweet texts:

\begin{enumerate}
  \item Tweet: ``The weather is so niceee!!! \#ILoveDarmstadt''
  \item Tweet: ``@rogerfederer is an 8 time Wimbledon champion! \#achievement''
\end{enumerate}

These will be preprocessed so that they can be used as an actual input to a neural network.

\paragraph{Word vectors}
\label{sub:word_vectors}

As mentioned above, an embedding is a function that maps from a specific
vocabulary to vectors with dimensionality $d$.
As a results, words are effectively placed in a vector space $\mathbb{R}^d$.
This allows examination of space structure and relationships between words of 
the vocabulary.
This thesis uses the pre-trained \textit{GloVe} word vectors by the \textit{Stanford
Natural Language Processing Group}\footcite{Pennington2014}\footnote{\url{https://nlp.stanford.edu/projects/glove/}}.
These were trained on a corpus of two billion tweets containing a total of
27 billion unique token from different languages, e.g., English, Arabic and
Japanese.
Here, a token can be a word or a symbol with a special meaning.
In detail, vectors were obtained by examining co-occurrence statistics in this
large data set.
Two special featues come with this set of word vectors.
Firstly, it is possible to quantify linguistic or semantic similarity between
words by calculating the Euclidian distance between them.
For example, the word pair ``frog-toad'' should have a smaller distance than
the pair ``frog-tiger'', because frogs and toads are both amphibians.
Secondly, calculating semantic distance allow to compare relatedness of word
pairs, so-called linear substructures.
As an example, the word pair ``man-woman'' should intuitively have similar distance
compared to the pair ``king-queen'', since the underlying concept to distinguish
the words is gender.
Using pre-existing word vectors removes the need to learn these parameters
at training time, thus speeding up the model development process.

\paragraph{Tokenization}
\label{sub:tokenization}

Making maximum use of pre-trained word vectors requires preprocessing the raw
tweet texts so that they are in conformity with existing standards set by
the creators.
Explicitly, \textit{GloVe} word vectors come with predefined tokens for tweet
components such as hashtags, URLs or user mentions, stylistic expressions such
as punctuation repetition or word elongation, and general concepts like numbers.
These tokens need to be recognized and marked in tweet texts.
In addtion, tokenization should preprocess the texts in a way that they can
later be separated into single tokens by simply splitting on whitespace.

\begin{table}
\begin{tabular}{rlll}
\toprule
Step & Description & Before & After \\
\midrule
1 & Mark URLs & ``https://t.co/IFt05'' & ``<url>'' \\
2 & Split words separated by slashes & ``fruits/vegetables'' & ``fruits / vegetables'' \\
3 & Mark user mentions & ``@rogerfederer'' & ``<user>'' \\
4 & Mark emoticons & ``:-)'' & ``<smile>'' \\
5 & Mark numbers & ``123'' & ``<number>'' \\
6 & Mark and split hashtags & ``\#SoNice'' & ``<hashtag> so nice'' \\
7 & Mark punctuation repetitions & ``!!!'' & ``! <repeat>'' \\
8 & Add spaces around punctuation & ``Watch out!'' & ``watch out !'' \\
9 & Mark elongated words & ``niceeeee'' & ``nice <elong>'' \\
10 & Mark allcaps words & ``IMPORTANT'' & ``important <allcaps>'' \\
11 & Remove unicode character & ``u00B3'' & `` '' \\
\bottomrule
\end{tabular}
\caption{Order of text preprocessing steps}
\label{tab:text_preprocessing}
\end{table}

Table~\ref{tab:text_preprocessing} shows the undertaken tweet preprocessing
steps for this thesis.
These are inspired by a Ruby script written by the GloVe creators\footnote{\url{https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb}}.
The code was ported to Python, including necessary fixes and expansions.
Special tokens often replace the original content, e.g., for URLs, user mentions,
numbers and emoticons.
The main reason behind this is that consuming algorithms are more capable of
making sense of the general occurrence of such components, rather than their
actual meaning.
For example, a neural network will not be able to derive the content of the
actual web pages behind URLs.
Stylistic expressions such as word elongations, punctuation repetitions or
allcaps words could be useful to learn about tweet sentiment since the author is 
obviously trying to highlight the specific passage.
Words that are transformed, e.g., by elongations or uppercasing, are converted to their original, lowercase
version for easier matching with the pre-trained word vectors.
Since hashtags are often written in camelcase form, they are split on uppercase
letters with the aim of understanding the meaning behind them.
Finally, spaces are added where needed, e.g., between slashes (obviously after
replacing URLs) or around punctuation.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{img/text_preprocessing_1}
  \caption{Tokenization example}
\label{fig:tokenization}
\end{figure}

Fig.~\ref{fig:tokenization} presents the results of applying the above listed
steps to the example tweets used in this section.
The tokenized texts are prepared for the next preprocessing steps, which
are described in the following paragraphs.
In practice, the derived tokenizations are often noisy due to individual tweeting
behaviors.
Unconvential writing often leads to the creation of unknown tokens and other
edge cases which are not covered by pre-trained word vectors.
This constitutes one of the reasons why huge amounts of data are usually needed
for NLP tasks.

\paragraph{Word index}
\label{sub:word_index}

\begin{figure}[h]
  \includegraphics[width=\textwidth]{img/text_preprocessing_2}
  \caption{Word index example}
\label{fig:word_index}
\end{figure}

\outline{First step of embedding creation: get a list of all occurring words}
\outline{Word index: map (key => word, value => unique ID)}
\outline{IDs are usually ordered by number of occurrences, s.th. most important
words can be extracted if number of words is limited for some reason}
\outline{List number of words for data sets + most common words}
\outline{Important: IDs are ascending (step size 1) and start 1}

\paragraph{Sequences}
\label{sub:sequences}

\begin{figure}[h]
  \includegraphics[width=\textwidth]{img/text_preprocessing_3}
  \caption{Text sequencing example}
\label{fig:text_sequencing}
\end{figure}

\outline{Second step: transform texts into sequence (i.e., list of word IDs)}
\outline{Split on whitespace (space, tab, newline)}
\outline{Lookup word ID in word index}
\outline{Zero-pad sequences according to specification (e.g., longest sequence)}

\paragraph{Embedding index}
\label{sub:embedding_index}

\begin{figure}[h]
  \includegraphics[height=7cm]{img/text_preprocessing_4}
  \caption{Embedding index example}
\label{fig:embedding_index}
\end{figure}

\outline{Third step: load pretrained word vectors from file}
\outline{Create a map (key: word, value: d-dimensional vector)}

\paragraph{Embedding matrix}
\label{sub:embedding_matrix}

\begin{figure}[h]
  \includegraphics[width=\textwidth]{img/text_preprocessing_5}
  \caption{Embedding matrix example}
\label{fig:embedding_matrix}
\end{figure}

\outline{Final step: create a matrix in which the i-th row represents the word
vector for the word with ID i from word index}
\outline{Dimensionality: (number of words) x d}
\outline{Enables simple lookup procedure for the network}
\outline{Zero row is default for unknown words (i.e., words that were not found in pretrained
word vectors)}


