\section{Introduction}

\subsection{Problem}
\label{sec:problem}

In 2011, Gartner senior vice president Peter Sondergaard made the following remark
about what is commonly referred to as the \textit{big data phenomenon}~\footnote{\url{https://www.gartner.com/newsroom/id/1824919}}:

\begin{quote}
  Information is the oil of the 21st century, and analytics is the combustion engine.
\end{quote}

Migrating transactions from the physical world to the internet has caused near
intangible increase in data availability.
More and more goods are purchased on the web, social networks shift communication
to the digital world, streaming platforms compete with more traditional ways
of media consumption and production is increasingly automated using smart robots.
These are just a few examples of processes that were digitized during the last
centuries.
As a result, more data is available, which comes in a variety of forms and
can often be accessed in real-time.
Organizations aim to make use of the increasing amount of information in the
context of data-driven decision making.
Working with large and diverse data sets requires the development of methods,
that are able to recognize patterns and derive recommendations for suitable
actions from them.
This process is also known as \textit{machine learning}, concerned with
enabling computer programs to learn from previous experiences.
In recent years, deep learning has emerged as a popular subfield of machine
learning, pushing state-of-the-art performance in fields like computer vision and
natural language processing.
Deep learning comprises the training of deep neural networks and has been found 
to work particularly well with unstructured data, e.g., images, texts or audio 
recordings.

Social media platforms have already been mentioned as one driver of the big
data phenomenon.
They enable users to create and share content with others, e.g., messages
(Facebook, Twitter), images (Instagram, Snapchat) or videos (YouTube).
Platforms then make use of collected data, e.g., by analyzing user behavior and
allowing advertisers to target specific market segments.
As social media has revolutionized the way people interact on the internet,
it also has practical implications for organizations.
Many companies actively maintain profiles on several platforms to enable
communication with and between customers.
Examples for social media activities are campaigns with the goal of brand
and community building or direct customer support.
Since its founding in 2006, Twitter has emerged as a microblogging platform
which is heavily used by individuals and organizations, especially in the
contexts of trend detection and news distribution.
Twitter users can post status messages, called \textit{tweets}, and interact
with other users via sharing or replying to their content.

This thesis is placed in the intersection of machine learning and social media,
namely engagement prediction.
This area of research is concerned with predicting the popularity of social
media content.
More specifically, this work aims to develop deep learning models which derive
estimates about the number of times a tweet will be shared and liked by other users.
Possible use cases of these models are systems for anomaly detection, e.g.,
early detection of trends or breaking news, or optimization of the content
creation process.
For example, decisions about the exact shaping of content or the time of
content publication could benefit from engagement prediction models.

\subsection{Objectives}
\label{sec:objectives}

\structure{Additional benefits from applying deep learning in contrast to prior work}
\outline{Forgo manual feature engineering, let model learn representation}
\outline{Use multi-class classification and regression to derive precise predictions}
\outline{Predict both retweets and favorites}
\outline{Learn advanced content features using modern NLP methods}
\outline{Apply models to diverse data sets for better generalization estimate}
\outline{Apply to user groups interested in this kind of model}
\outline{Find out about data requirements, apply to differently sized data sets}

\structure{Deployable model}
\outline{Minimal preprocessing}
\outline{Quick inference}
\outline{Ad-hoc prediction, not reliant on retweet cascades}

\subsection{Approach}
\label{sec:approach}

\structure{Data}
\outline{Build three different user groups - celebrities, politicians, companies}
\outline{Collect all tweets for given time frame as training data}
\outline{Data sets should employ diverse engagement distributions}
\outline{Additionally, construct bigger data set containing all user groups
and tweets from longer time frame}
\outline{For all data sets, extract structured features according to prior work
in this area}
\outline{Also, extract raw tweet texts as addtional, unstructured inputs}

\structure{Developed models}
\outline{First step: Start with linear baseline model as simplest model type}
\outline{Uses structured input features}
\outline{Second step: Develop deep feedforward neural networks, solely relying
on structured inputs as well}
\outline{Think of it as simplest form of neural network}
\outline{Third step: Develop more sophisticated multi-input neural network,
which takes both structured and unstructured inputs}
\outline{Compare results across model types and data sets}

\structure{Structure of this thesis}
