\section{Introduction}

\subsection{Problem}
\label{sec:problem}

In 2011, Gartner senior vice president Peter Sondergaard made the following remark
about what is commonly referred to as the big data phenomenon~\footnote{\url{https://www.gartner.com/newsroom/id/1824919}}:

\begin{quote}
  Information is the oil of the 21st century, and analytics is the combustion engine.
\end{quote}

Migrating transactions from the physical world to the internet has caused a near
intangible increase in data availability.
More and more goods are purchased on the web, social networks shift communication
to the digital world, streaming platforms compete with more traditional ways
of media consumption and production is increasingly automated using smart robots.
These are just a few examples of processes that were digitized during the last
centuries.
As a result, more data is available, which comes in a variety of forms and
can often be accessed in real-time.
Organizations aim to make use of the increasing amount of information in the
context of data-driven decision making.
Working with large and diverse data sets requires the development of methods,
that are able to recognize patterns and derive recommendations for suitable
actions from them.
This process is also known as \textit{machine learning}, concerned with
enabling computer programs to learn from previous experiences.
In recent years, deep learning has emerged as a popular subfield of machine
learning, pushing state-of-the-art performance in fields like computer vision and
natural language processing.
Deep learning comprises the training of deep neural networks and has been found 
to work particularly well with unstructured data, e.g., images, texts or audio 
recordings.

Social media platforms have already been mentioned as one driver of the big
data phenomenon.
They enable users to create and share content with others, e.g., messages
(Facebook, Twitter), images (Instagram, Snapchat) or videos (YouTube).
Platforms then make use of collected data, e.g., by analyzing user behavior and
allowing advertisers to target specific market segments.
As social media has revolutionized the way people interact on the internet,
it also has practical implications for organizations.
Many companies actively maintain profiles on several platforms to enable
communication with and between customers.
Examples for social media activities are campaigns with the goal of brand
and community building or direct customer support.
Since its founding in 2006, Twitter has emerged as a microblogging platform
which is heavily used by individuals and organizations, especially in the
contexts of trend detection and news distribution.
Twitter users can post status messages, called \textit{tweets}, and interact
with other users via sharing or replying to their content.

This thesis is placed in the intersection of machine learning and social media,
namely engagement prediction.
This area of research is concerned with predicting the popularity of social
media content.
More specifically, this work aims to develop deep learning models which derive
estimates about the number of times a tweet will be shared and liked by other users.
Possible use cases of these models are systems for anomaly detection, e.g.,
early detection of trends or breaking news, or optimization of the content
creation process.
For example, decisions about the exact shaping of content or the time of
content publication could benefit from engagement prediction models.

\subsection{Objectives}
\label{sec:objectives}

Experiments conducted in this thesis examine whether deep learning methods are
suitable for tweet engagement prediction.
Previous work in this area is mainly focussed on developing simple statistical
models, which rely on manual feature engineering.
Especially in the context of extracting features about the tweet content, this
often requires massive amounts of data preprocessing.
In contrast to that, deep neural networks are capable of learning their own 
feature representations, even from unstructured data such as text.
Thus, less preprocessing and feature engineering is required when applying
these models.
Moreover, tackled tasks in the literature are oftentimes simplified and not of 
practical relevance, e.g., a simple prediction whether a message will be 
retweeted or not.
An additional shortcoming is the focus on retweets as the sole considered
engagement metric.
Models developed in the context of this thesis aim to produce useful predictions
for both retweet and favorite counts.
Furthermore, the models are applied to a diverse set of tweet data in order
to derive a good estimate of their generalization capabilities.
Here, all examined tweets stem from user groups who could potentially be interested
in deploying such models, i.e., public figures and organizations.
Employing the models on differently sized data sets should also enable estimates
about the data requirements for training deep learning models on such a complex
task.

In addition to coming up with satisfactory predictions about tweet engagement,
this thesis puts an emphasis on developing practically oriented models.
This means developed deep learning models should be deployable in a real-world
setting.
This includes the ability to make real-time predictions for previously unseen
data.
In order to account for these requirements, necessary preprocessing of incoming
tweet objects needs to be kept to a minimum.
As the prediction should happen in an ad-hoc fashion at the time of creation,
no additional data requests can be made, i.e., all input features for the model
are extracted from the tweet itself.
The number and nature of usable features is therefore limited by the data source,
e.g., the public Twitter API.

\subsection{Approach}
\label{sec:approach}

As mentioned previously, all developed models are tested on a variety of data
sets, which differ in size and engagement distributions.
Here, fixed user groups are established, with accounts coming from a variety
of fields, e.g., politics, entertainment, sports and industry.
For all user groups, all tweets for a given time frame are collected.
The resulting tweet collections then serve as training and validation data for
the developed models.
It is ensured that tweets from all popularity levels are contained in the
collected data sets.
The data sets naturally differ in size, from 40,000 to more than 800,000 examples.
This allows examinations of data requirements for deep neural network training
on this specific task.
Preprocessing of the raw data sets is limited to the extraction of structured
features and preparation of the tweet content, which is of unstructured nature.
The derived features then serve as a direct input to the developed models.

The model types are developed and trained on each data set.
Firstly, linear models are trained in order to establish baseline performance
measures.
These use structured input features, i.e., data that can be stored in
relational format.
Examples for structured features are the number of followers and friends, time
of tweet creation or the number of contained URLs.
Secondly, deep feedforward neural networks are trained on the same exact input
data.
This type of model can be thought of as the simplest form of a neural network.
Thirdly, more sophisticated neural network architectures are tested, where
the minimally preprocessed tweet content serves as an additional input.
Finally, results are compared across model types and data sets, taking into
account the previously established objectives.

The structure of this thesis is outlined in the following.
Chapter~\ref{ch:background} covers theoretical concepts of deep learning and
social networks, which are necessary for the comprehension of later chapters.
Afterwards, the methodology used in this work is explained in Chapter~\ref{ch:methodology}.
Results of all developed deep learning models are presented in Chapter~\ref{ch:results},
which also analyzes collected data sets in a descriptive manner.
The main contributions of this thesis are outlined in Chapter~\ref{ch:conclusion},
before opportunities for future research are described in Chapter~\ref{ch:future_work}.
