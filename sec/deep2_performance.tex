\subsubsection{Model performance}
\label{sub:comb_performance}

The final section of this chapter presents results of experiments with multi-input
deep neural networks conducted in the scope of this work.
In the following paragraphs, classification and regression performance on all
data sets is listed and analyzed in detail.
The concluding results chapter (ch.~\ref{sec:res_summary}) will then put these
outcomes in relation to previously trained models.

\begin{table}
  \begin{tabular}{lrrrrrrr}
    \toprule
    & \multicolumn{4}{l}{Classification} & \multicolumn{3}{l}{Regression} \\
    \midrule
    Data set & CCE & Acc & Ret Acc & Fav Acc & MAE & Ret MAE & Fav MAE \\
    \midrule
    Celebrities & 0.80 & 65.16\% & 61.91\% & 68.40\% & 3,286.16 & 1,438.98 & 5,133.35 \\
    Politicians & 0.82 & 65.34\% & 64.22\% & 66.46\% & 447.61 & 262.78 & 632.44 \\
    Companies & 0.62 & 73.52\% & 75.88\% & 71.15\% & 19.41 & 10.99 & 27.84 \\
    Combined & 0.76 & 68.42\% & 70.05\% & 66.79\% & 401.52 & 201.70 & 601.33 \\
    \bottomrule
  \end{tabular}
  \caption{Summary of results for multi-input deep neural networks}
  \label{tab:deep2_results}
\end{table}

Table~\ref{tab:deep2_results} summarizes multi-input model results for both
engagement classification and regression.
As with all models in this work, metrics are calculated on held-out validation
examples.
Overall classification performance is similar for all data sets, as CCE loss values
even out between 0.62 and 0.82.
These results account for an overall classification accuracy, i.e., percentage
of correctly classified examples, of 65 to 75\%.
Best performance is achieved on the company data set, which is also the most
imbalanced data set when looking at class distributions (see ch.~\ref{sec:engagement_stats}).
In contrast to that, performance on the smaller celebrity and politician data sets
worsens.
Tweets from the larger, combined data sets are classified correctly 68\% of the
time.
Thus, classification performance seems not to be dependent on data set size alone.
Notable differences in retweet and favorite classification accuracy exist,
with the spread being widest for celebrity (6.49\%) and smallest for
politician tweets (2.24\%).
It has to be mentioned, that accuracies of more than 90\% were achievable on all
data sets when ignoring overfitting and solely focussing on training loss.
This once again points to the difficulty of learning generalizable models for
the complex task of engagement prediction.
As expected, MAE values for the regression task are of varying magnitude.
Predictions on more narrow engagement distributions (e.g., company data set)
are most precise than their counterparts on data sets containing many outliers
(e.g., celebrity data set).
As with classifications, notable differences between retweet and favorite prediction
can be identified.
In general, retweet predictions are most accurate, with favorite error values
being about three times bigger on all data sets.
According to the procudure applied in previous results chapters, both classification
and regression performance are analyzed in more detail in the following paragraphs.

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
  \includegraphics[width=.95\linewidth]{img/celeb_d2_cm_retweets}
  \caption{Celebrity data set}
  \label{fig:retw_distr_sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \includegraphics[width=.95\linewidth]{img/polit_d2_cm_retweets}
  \caption{Politician data set}
  \label{fig:retw_distr_sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \includegraphics[width=.95\linewidth]{img/corp_d2_cm_retweets}
  \caption{Company data set}
  \label{fig:retw_distr_sub3}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \includegraphics[width=.95\linewidth]{img/comb_d2_cm_retweets}
  \caption{Combined data set}
  \label{fig:retw_distr_sub4}
\end{subfigure}%
\caption{Confusion matrices for multi-input deep neural networks}
\label{fig:d2_cm}
\end{figure}

\structure{Description of classification results}
\outline{Specialized data sets: class accuracies above 50\% for three of four
classes}
\outline{Best performance on company data set mainly stems from strong performance
on most popular classes (acc around 80\%)}
\outline{Less common classes are still predicted poorly (sometimes not at all)
on these data sets}
\outline{Combined data set: perf is more promising, five of six classes are
predicted with acc higher than 50\%}
\outline{Only class six is predicted poorly}
\outline{Class 2 is predicted correctly 85\% of the time, possible indication
of data requirement for labeled examples from one class}
\outline{Overall: Still biased predictions, misclassifications are directed towards
most popular class}

\begin{table}
  \begin{tabular}{lrrrrrrr}
    \toprule
    & \multicolumn{7}{c}{Actual retweets} \\
    \midrule
    Data set & 0 & 1-10 & 10-100 & 100-1k & 1-10k & 10-100k & >100k \\
    \midrule
    Celebrities & 28.7 & 68.6 & 231.0 & 445.5 & 2,048.6 & 25,751.3 & 242,061.9 \\
    Politicians & 6.3 & 9.1 & 47.9 & 299.2 & 2,087.5 & 16,362.4 & - \\
    Companies & 0.5 & 3.2 & 17.8 & 185.4 & 2,099.5 & 13,939.7 & - \\
    Combined & 0.5 & 6.2 & 46.6 & 253.3 & 1,907.6 & 21,010.1 & 67,083.3 \\
    \bottomrule
    \toprule
    & \multicolumn{7}{c}{Actual favorites} \\
    \midrule
    Data set & 0 & 1-10 & 10-100 & 100-1k & 1-10k & 10-100k & >100k \\
    \midrule
    Celebrities & 10.1 & 27.4 & 339.2 & 795.4 & 2,047.8 & 23,117.3 & 161,387.5 \\
    Politicians & 30.2 & 20.5 & 70.7 & 267.2 & 2,056.0 & 18,204.5 & 108,637.6 \\
    Companies & 0.6 & 4.2 & 16.9 & 228.7 & 1,710.6 & 19,358.1 & - \\
    Combined & 1.0 & 6.5 & 46.5 & 403.8 & 1,878.7 & 18,932.1 & 89,079.0 \\
    \bottomrule
  \end{tabular}
  \caption[Detailed regression results for multi-input deep neural networks]{Mean absolute errors for specific ranges of actual engagement}
  \label{tab:d1_regression_eval}
\end{table}

\structure{Description of regression results}
\outline{Errors are still increasing with actual engagement}
\outline{Most imbalanced company data set shows lowest errors for many classes}
\outline{Zero values are predicted fairly well for company and combined data set}
\outline{Outliers have strong influence on mean calculation}
