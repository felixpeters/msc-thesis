\subsection{Summary}
\label{sec:res_summary}

Over the course of the previous sections, three different model types were
applied to the problem of engagement prediction for tweets.
This concluding section of the results chapter compares results of these models
across all collected data sets.
Distinct comparisons are undertaken for classification and regression tasks.
In addition, plausible explanations for differences in detected performance are
given.

\begin{table}
\begin{tabular}{llrrr}
\toprule
\multicolumn{2}{c}{Data set} & \multicolumn{3}{c}{Model type} \\
\midrule
Name & Examples & Linear model & Feedforward DNN & Multi-input DNN \\
\midrule
Celebrities & 46,986 & 48.84\% & 70.29\% & 65.16\% \\
Politicians & 94,919 & 54.17\% & 64.75\% & 65.34\% \\
Companies & 254,059 & 59.64\% & 70.07\% & 73.52\% \\
Combined & 863,004 & 51.69\% & 55.50\% & 68.42\% \\
\bottomrule
\end{tabular}
\caption{Summary of results for classification task}
\label{tab:summary_classification}
\end{table}

Table~\ref{tab:summary_classification} presents the evolution of results for linear,
deep feedforward and multi-input classifiers developed in this work.
For each model and data set, the overall accuracy of predictions is listed.
It can be stated that deep feedforward neural networks show better classification
performance linear models on all data sets.
Thus, for the specific task of engagement prediction, models using structured
input data benefit from adding layers of abstraction.
This is intuitive, since this process allows learning more complex features.
Because of the black-box nature of neural networks, these performance gains
come at the cost of interpretability.
Consequently, it is not possible to derive an intuitive explanation of the learned
features directly from the network.
Structured input features in this work mainly comprise contextual features like
follower count, which point to the overall popularity and activity of a tweet
author.
Adding the tweet content as an additional input, i.e., creating a multi-input
neural network, results in performance increases for all data sets except
celebrity tweets.
A possible explanation of the performance decrease for this data set is its rather small number of
training examples, which makes it difficult for the network to extract meaningful
features from the additional, unstructured inputs.
However, this type of model still performs better than simple linear models
on this data set.
It can be observed that the magnitude of performance gain increases with the
overall size of data sets.
Adding texts as further inputs leads to nearly 13\% increase in classification
accuracy on the combined data set, containing tweet from all three user groups
over the course of two full years.
This dependency on the number of available training examples seems to be logical,
since intuitively more powerful features can be learned from a larger set of words and
sentence structures.
Overall, classification experiments in this work indicate that
deep learning can help to improve the accuracy of engagement predictions.
Nevertheless, the results leave room for further improvements and thus point
to research opportunities in this area.
For example, previous analysis of classification results showed that class accuracy for less
common classes increases the more labeled examples are present at training time (see ch.~\ref{sub:comb_performance}).
This suggests that adding more tweet data, especially for less popular classes,
could further increase prediction accuracy and consequently overall model performance.
Since examples for very popular tweets, e.g., with more than 10,000 retweets, are
rare, a realignment of classes would also be a plausible action.

\begin{table}
\begin{tabular}{llrrr}
\toprule
\multicolumn{2}{c}{Data set} & \multicolumn{3}{c}{Model type} \\
\midrule
Name & Examples & Linear model & Feedforward DNN & Multi-input DNN \\
\midrule
Celebrities & 46,986 & 3,490.21 & 2,345.91 & 3,286.16 \\
Politicians & 94,919 & 482.81 & 414.68 & 447.61 \\
Companies & 254,059 & 24.66 & 20.57 & 19.41 \\
Combined & 863,004 & 579.26 & 549.93 & 401.52 \\
\bottomrule
\end{tabular}
\caption{Summary of results for regression task}
\label{tab:summary_regression}
\end{table}

Results for directly predicting retweet and favorite counts prove to be similar
to the classification observations.
Table~\ref{tab:summary_regression} list weighted mean absolute errors for all model types
and data sets.
Again, deep feedforward networks provide a significant performance gain over
linear regression models.
Adding tweet content as an additional input results in further improvements for
company and combined data set.
For the latter data set, a 27\% decrease in loss values can be observed, which
points to the predictive power of features derived from these additional inputs.
It has be stated that the target space of actual engagement counts is broad, spanning
from tweets with no engagement at all to popular messages with hundreds of thousands
of retweets and favorites.
Since the number of examples on each part of this spectrum varies between all
data sets, the individual mean error values are hardly comparable.
Moreover, the mean calculation complicates interpretability of the metric.
Intuitively, an error of 400 should be evaluated differently for retweet counts
of zero and 10,000.
All in all, above results show that regression models for engagement prediction
benefit from additional features, which are extracted from the preprocessed
tweet content.
Possible approaches for further model development are discussed in Chapter~\ref{ch:future_work}.

This section concludes the presentation of results from experiments undertaken
in the context of this thesis.
