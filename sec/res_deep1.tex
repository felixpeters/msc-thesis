\subsection{Deep Models on Structured Data}
\label{sec:deep1}

Linear models directly map from inputs to predictions.
Assuming the inputs stay the same, the next step in neural network evolution
should be to add hidden layers.
The reasoning behind this is the increased capability of the model to learn intermediate
feature representations, which in turn should enable more accurate predictions.
This section examines application of deep feedforward networks to the problem
of engagement prediction.
Model architectures are introduced in Chapter~\ref{sub:deep1_architecture}, 
before the training process is detailled in Chapter~\ref{sub:deep1_training}.
Finally, results and accompanying analysis are presented in Chapter~\ref{sub:deep1_performance}.

\subsubsection{Model architecture}
\label{sub:deep1_architecture}

Model architecture for the deep feedforward networks is similar to linear models
in that model inputs and outputs are the same.
The main difference is the addition of hidden layers inbetween inputs and outputs.
Fully-connected feedforward layers (see ch.~\ref{sub:dl_concepts}) are chosen,
because they constitute the sole reasonably applicable layer architecture.
More complex layer architectures like recurrent or convolutional layers require
temporal or local relations between input features.
Examples for such relations would be images (local relations between pixels)
or time-series data (obvious temporal relation).
The main question that arises is how many hidden layers should be inserted.
Additionally, the number of neurons in hidden layers has to be determined.
No strict rules exist to answer this question, but heuristics have been developed
by long-time deep learning practicioners~\footnote{\url{http://www.heatonresearch.com/2017/06/01/hidden-layers.html}}.
In any case, experimentation is required to find a suitable architecture for a distinct problem setting.
Thus, several architectures were tested for model selection in this work.
Obvious evaluation criteria are model performance on training and validation data,
as well as total training time.

\begin{table}
\begin{tabular}{llrr}
\toprule
Hidden layers & Hidden units & Classification loss & Regression loss \\
\midrule
2 & 16 & 0.8203 & 2,593.2 \\
2 & 32 & 0.7502 & 2,520.2 \\
2 & 64 & 0.7346 & 2,509.9 \\
3 & 16 & 0.7888 & 2,529.9 \\
3 & 32 & 0.7430 & 2,557.3 \\
3 & 64 & 0.7160 & 2,465.1 \\
4 & 16 & 0.7792 & 2,588.5 \\
4 & 32 & 0.7261 & 2,477.5 \\
4 & 64 & \textbf{0.7088} & 2,435.8 \\
5 & 64 & 0.7128 & \textbf{2,386.1} \\
6 & 64 & 0.7016 & 2,410.1 \\
\bottomrule
\end{tabular}
\caption{Summary of model selection results}
\label{tab:dm1_selection_results}
\end{table}

Experimentation for classification and regression models was conducted on the
celebrity data set, because it contains the smallest number of training examples
and is thus most prone to overfitting.
In order to preserve comparability, the same settings were used for each architecture.
Models were trained for 50 (classification) respectively 100 (regression) epochs
on identical training examples.
Tested variants included two to six hidden layers with the same number of hidden
units in each layer, namely 16, 32 and 64 neurons.
Some architectures for very deep networks (five or six hidden layers) were omitted
when it became obvious that layers with 64 neurons showed the most promising
performance.
Table~\ref{tab:dm1_selection_results} summarizes results for the experiments.
As expected, increases in both number of hidden layers and units improve
performance.
However, adding units yields generally leads to higher performance increases than
simply adding layers.
In addition, adding more hidden layers increases total training time more than
adding units.
Including more than four hidden layers only leads to minor performance improvement for
the classification setting.
Also, a more unstable training process was observed, sometimes resulting in overfitting.
Hence, the configuration consisting of four layers with 64 neurons each was
chosen for models in this thesis.
The regression setting proved to be a bit more robust regarding the number of
hidden layers, so that an additional layer could be inserted here.

\begin{figure}[h]
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=.95\linewidth]{img/deep_1_class_architecture}
  \caption{Classification model}
  \label{fig:deep1_architecture_1}
\end{subfigure}%
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=.95\linewidth]{img/deep_1_regr_architecture}
  \caption{Regression model}
  \label{fig:deep1_architecture_2}
\end{subfigure}
\caption{Architecture of deep feedforward network}
\label{fig:deep1_architecture}
\end{figure}

Fig.~\ref{fig:deep1_architecture} shows the final neural network architectures
for this section.
Input features and output types (including classes) are identical to the ones
used in linear models (see ch.~\ref{sub:lin_architecture}).
As before, inputs are normalized using batch normalization, which is also applied
after each fully-connected layer.
Citing Chapter~\ref{sub:dl_developments}, batch normalization speeds up the
training process by decreasing the range of weight values without sacrificing
model performance.
Moreover, it helps to avoid overfitting, which is more prone to occur in models
containing many learnable parameters.

\subsubsection{Training process}
\label{sub:deep1_training}

\outline{Setting is mostly unchanged: same training/validation split, optimizer
and cost functions}
\outline{Only change: regression model trained for only 100 iterations since they
converge faster}

\subsubsection{Model performance}
\label{sub:deep1_performance}

\begin{table}
  \begin{tabular}{lrrrrrrr}
    \toprule
    & \multicolumn{4}{l}{Classification} & \multicolumn{3}{l}{Regression} \\
    \midrule
    Data set & CCE & Acc & Ret Acc & Fav Acc & MAE & Ret MAE & Fav MAE \\
    \midrule
    Celebrities & 0.71 & 70.29\% & 66.30\% & 74.28\% & 2,345.85 & 1,165.19 & 3,526.51 \\
    Politicians & 0.81 & 64.75\% & 63.37\% & 66.12\% & 424.05 & 247.40 & 600.69 \\
    Companies & 0.72 & 70.07\% & 72.38\% & 67.76\% & 19.72 & 10.92 & 28.52 \\
    Combined & 1.03 & 55.50\% & 57.75\% & 53.25\% & 549.93 & 210.71 & 889.16 \\
    \bottomrule
  \end{tabular}
  \caption{Summary of results for deep feedforward network}
  \label{tab:deep1_results}
\end{table}

\outline{Overall performance increases (cost function lower for both tasks
across all data sets)}
\outline{Additional information: models start to overfit slightly (esp. company
classification => training loss is 0.64)}
\outline{Classification accuracy improvements between 10 and 20 \% (celebrity performance
improvements are strongest), 65 to 70\% of validation examples are classified correctly}
\outline{Improvements are slightly higher for favorite classification (except for politicians)}
\outline{Relation between RetAcc and FavAcc unchanged}
\outline{Interesting: classification works best for celebrity data (previously worst performance)}
\outline{MAE results lowered by 12 (politicians) to 33\% (celebrities)}
\outline{Improvements mostly driven by strong decreases in FavMAE (decreases
higher across the board)}
\outline{More details on classification and regression performance in the following}

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
  \includegraphics[width=.95\linewidth]{img/celeb_d1_cm_retweets}
  \caption{Celebrity data set}
  \label{fig:retw_distr_sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \includegraphics[width=.95\linewidth]{img/polit_d1_cm_retweets}
  \caption{Politician data set}
  \label{fig:retw_distr_sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \includegraphics[width=.95\linewidth]{img/corp_d1_cm_retweets}
  \caption{Company data set}
  \label{fig:retw_distr_sub3}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \includegraphics[width=.95\linewidth]{img/comb_d1_cm_retweets}
  \caption{Combined data set}
  \label{fig:retw_distr_sub4}
\end{subfigure}%
\caption{Confusion matrices for deep feedforward classification models}
\label{fig:d1_cm}
\end{figure}

\outline{More correctly classified examples (see main diagonal)}
\outline{Celebrities: Classes around median are predicted more accurately than border classes}
\outline{Celebrities: Not biased towards biggest class, misclassifications are `better'}
\outline{Sames is true for politicians}
\outline{Companies: model is good at predicting first two classes}
\outline{Companies: better (but not good) performance on classes 3 and 4}
\outline{Companies: classes 5 and 6 are still never predicted}

\begin{table}
  \begin{tabular}{lrrrrrrr}
    \toprule
    & \multicolumn{7}{c}{Actual retweets} \\
    \midrule
    Data set & 0 & 1-10 & 10-100 & 100-1k & 1-10k & 10-100k & >100k \\
    \midrule
    Celebrities & - & 51.9 & 161.4 & 701.3 & 3,884.9 & 21,557.9 & - \\
    Politicians & 46.0 & 109.7 & 184.5 & 781.0 & 3,973.1 & - & - \\
    Companies & 1.5 & 5.9 & 49.5 & 605.7 & - & - & - \\
    Combined & 0.4 & 5.2 & 32.0 & 233.6 & 2,088.3 & 21,919.0 & 86,217.9 \\
    \bottomrule
    \toprule
    & \multicolumn{7}{c}{Actual favorites} \\
    \midrule
    Data set & 0 & 1-10 & 10-100 & 100-1k & 1-10k & 10-100k & >100k \\
    \midrule
    Celebrities & - & 7.4 & 268.5 & 638.1 & 3,139.2 & 19,212.0 & 45,031.3 \\
    Politicians & 92.6 & 130.4 & 234.1 & 864.5 & 3,528.9 & 12,167.6 & - \\
    Companies & 6.5 & 6.6 & 39.7 & 743.1 & 1,314.0 & - & - \\
    Combined & 0.4 & 4.4 & 32.1 & 355.8 & 3,279.5 & 26,871.1 & 169,920.5 \\
    \bottomrule
  \end{tabular}
  \caption{Mean absolute errors for specific ranges of actual engagement}
  \label{tab:d1_regression_eval}
\end{table}

\outline{Using MAE requires mean calculation => need for more detailed performance
evaluation}
\outline{Errors need to be considered in relation to magnitude of actual counts}
\outline{Table shows MAE for specific ranges of actual engagement counts}
\outline{Dash denotes absence of validation data for this range}
\outline{Observation: MAE increases with rising engagement counts}
\outline{Order of magnitudes vary drastically (can be expected)}

