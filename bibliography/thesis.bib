@inproceedings{Abadi2016,
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI '16)},
pages = {265--283},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
volume = {16},
year = {2016}
}
@inproceedings{Bakshy2011,
author = {Bakshy, Eytan and Hofman, Jake M. and Mason, Winter A. and Watts, Duncan J.},
booktitle = {Proceedings of the fourth ACM international conference on Web search and data mining},
number = {January},
pages = {65--74},
title = {{Everyone's an influencer: quantifying influence on Twitter}},
year = {2011}
}
@article{Barabasi2003,
author = {Barab{\'{a}}si, Albert-L{\'{a}}szl{\'{o}} and Bonabeau, Eric},
journal = {Scientific American},
number = {1},
pages = {50--59},
title = {{Scale-free networks}},
volume = {3},
year = {2003}
}
@inproceedings{Bertinetto2016,
author = {Bertinetto, Luca and Valmadre, Jack and Henriques, Jo{\~{a}}o F. and Vedaldi, Andrea and Torr, Philip H. S.},
booktitle = {European conference on computer vision},
number = {October},
pages = {850--865},
title = {{Fully-Convolutional Siamese Networks for Object Tracking}},
year = {2016}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
publisher = {Springer},
address = {Berlin},
edition = {1},
pages = {738},
title = {{Pattern Recognition And Machine Learning}},
year = {2006}
}
@inproceedings{Golder,
author = {Boyd, Danah and Golder, Scott and Lotan, Gilad},
booktitle = {2010 43rd Hawaii International Conference on System Sciences},
month = {January},
pages = {1--10},
publisher = {IEEE},
title = {{Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter}},
year = {2010}
}
@article{Brynjolfsson2017,
author = {Brynjolfsson, Erik and Mcafee, Andrew},
journal = {Harvard Buisness Review},
pages = {1--8},
title = {{The Business of Artificial Intelligence}},
year = {2017}
}
@misc{Cho2014,
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
number = {31.03},
year = {2014}
}
@inproceedings{Conneau2016,
author = {Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"{i}}c and Lecun, Yann},
journal = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
pages = {1107--1116},
title = {{Very Deep Convolutional Networks for Text Classification}},
volume = {1},
year = {2017}
}
@article{Culnan2010,
author = {Culnan, Mary J. and McHugh, Patrick and Zubillaga, Jesus I.},
journal = {MIS Quarterly Executive},
number = {4},
pages = {243--259},
title = {{How large US companies can use Twitter and other social media to gain business value}},
volume = {9},
year = {2010}
}
@article{Fukushima1980,
author = {Fukushima, Kunihiko},
journal = {Biological Cybernetics},
number = {4},
pages = {193--202},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{Fukushima1975,
author = {Fukushima, Kunihiko},
journal = {Biological Cybernetics},
number = {3-4},
pages = {121--136},
title = {{Cognitron: A self-organizing multilayered neural network}},
volume = {20},
year = {1975}
}
@article{Gatys2015,
author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
title = {{A Neural Algorithm of Artistic Style}},
url = {http://arxiv.org/abs/1508.06576},
urldate = {31.03.2018},
year = {2015}
}
@inproceedings{Girshick2012,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
month = {November},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@misc{Go2009,
author = {Go, Alec and Bhayani, Richa and Huang, Lei},
journal = {CS224N Project Report, Stanford},
number = {12},
pages = {12},
title = {{Twitter sentiment classification using distant supervision}},
url = {http://www.stanford.edu/{~}alecmgo/papers/TwitterDistantSupervision09.pdf},
urldate = {02.04.2018}
volume = {1},
year = {2009}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {MIT Press},
pages = {800},
title = {{Deep Learning}},
volume = {1},
year = {2016}
}
@inproceedings{Goodfellow2014,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
title = {{Generative Adversarial Nets}},
year = {2014}
number = {27}
}
@article{Granovetter1973,
author = {Granovetter, Mark},
journal = {The American Journal of Sociology},
number = {6},
pages = {1360--1380},
title = {{The Strength of Weak Ties}},
volume = {78},
year = {1973}
}
@article{Hahnioser2000,
author = {Hahnioser, Richard H.R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
journal = {Nature},
number = {6789},
pages = {947--951},
title = {{Digital selection and analogue amplification coexist in a cortex- inspired silicon circuit}},
volume = {405},
year = {2000}
}
@inproceedings{He2017,
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
booktitle = {International Conference on Computer Vision (ICCV)},
pages = {2980--2988},
title = {{Mask R-CNN}},
year = {2017}
}
@inproceedings{He2016,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
month = {December},
pages = {770--778},
title = {{Deep Residual Learning for Image Recognition}},
year = {2016}
}
@article{Holden2006,
author = {Hinton, G. E.},
journal = {Science},
number = {5786},
pages = {504--507},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
volume = {313},
year = {2006}
}
@article{Hinton2006,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural computation},
number = {7},
pages = {1527--54},
title = {{A fast learning algorithm for deep belief nets.}},
volume = {18},
year = {2006}
}
@misc{Hinton2012a,
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
pages = {1--18},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
urldate = {02.04.2018}
year = {2012}
}
@article{Hinton2012,
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel Rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {82--97},
title = {{Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups}},
volume = {29},
year = {2012}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, Jurgen J{\"{u}}rgen},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long short-term memory}},
volume = {9},
year = {1997}
}
@article{Hoffman2010,
author = {Hoffman, Dl D.L. Donna L and Fodor, Marek},
journal = {MIT Sloan Management Review},
number = {1},
pages = {41--49},
title = {{Can You Measure the ROI of Your Social Media Marketing?}},
volume = {52},
year = {2010}
}
@inproceedings{Hong2011,
author = {Hong, Liangjie and Dan, Ovidiu and Davison, Brian D.},
booktitle = {Proceedings of the 20th international conference companion on World wide web},
pages = {57--58},
title = {{Predicting popular messages in Twitter}},
year = {2011}
}
@inproceedings{Ioffe2015,
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {International conference on machine learning},
pages = {448--456},
title = {{Batch Normalization}},
year = {2015}
}
@article{Jordan2015,
author = {Jordan, M. I. and Mitchell, T. M.},
journal = {Science},
number = {6245},
pages = {255--260},
title = {{Machine learning: Trends, perspectives, and prospects}},
volume = {349},
year = {2015}
}
@article{Kaplan2010,
author = {Kaplan, Andreas M. and Haenlein, Michael},
journal = {Business Horizons},
number = {1},
pages = {59--68},
title = {{Users of the world, unite! The challenges and opportunities of Social Media}},
volume = {53},
year = {2010}
}
@article{Karpathy2017,
author = {Karpathy, Andrej and Fei-Fei, Li},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {4},
pages = {664--676},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
volume = {39},
year = {2017}
}
@article{Kim2012,
author = {Kim, Angella J. and Ko, Eunju},
journal = {Journal of Business Research},
number = {10},
pages = {1480--1486},
publisher = {Elsevier Inc.},
title = {{Do social media marketing activities enhance customer equity? An empirical study of luxury fashion brand}},
volume = {65},
year = {2012}
}
@inproceedings{Kim2015,
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
booktitle = {AAAI},
pages = {2741--2749},
title = {{Character-Aware Neural Language Models}},
year = {2016}
}
@misc{Kingma2014a,
author = {Kingma, Diederik P. and Ba, Jimmy},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
urldate = {02.04.2018}
year = {2014}
}
@inproceedings{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances In Neural Information Processing Systems},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@inproceedings{Kupavskii2012,
author = {Kupavskii, Andrey and Ostroumova, Liudmila and Umnov, Alexey and Usachev, Svyatoslav and Serdyukov, Pavel and Gusev, Gleb and Kustarev, Andrey},
booktitle = {Proceedings of the 21st ACM international conference on Information and knowledge management},
number = {October},
pages = {2335--2338},
title = {{Prediction of retweet cascade size over time}},
year = {2012}
}
@inproceedings{Kwak2010,
author = {Kwak, Haewoon and Lee, Changhyun and Park, Hosung and Moon, Sue},
booktitle = {Proceedings of the 19th international conference on world wide web},
pages = {591--600},
title = {{What is Twitter, a Social Network or a News Media?}},
year = {2010}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
journal = {Nature},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@inproceedings{LeCun1998,
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
booktitle = {Proceedings of the IEEE},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@inproceedings{Lee2014,
author = {Lee, Kyumin and Mahmud, Jalal and Chen, Jilin and Zhou, Michelle and Nichols, Jeffrey},
booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
pages = {247--256},
title = {{Who Will Retweet This?: Automatically Identifying and Engaging Strangers on Twitter to Spread Information}},
year = {2014}
}
@article{Mangold2009,
abstract = {The emergence of Internet-based social media has made it possible for one person to communicate with hundreds or even thousands of other people about products and the companies that provide them. Thus, the impact of consumer-to-consumer communications has been greatly magnified in the marketplace. This article argues that social media is a hybrid element of the promotion mix because in a traditional sense it enables companies to talk to their customers, while in a nontraditional sense it enables customers to talk directly to one another. The content, timing, and frequency of the social media-based conversations occurring between consumers are outside managers' direct control. This stands in contrast to the traditional integrated marketing communications paradigm whereby a high degree of control is present. Therefore, managers must learn to shape consumer discussions in a manner that is consistent with the organization's mission and performance goals. Methods by which this can be accomplished are delineated herein. They include providing consumers with networking platforms, and using blogs, social media tools, and promotional tools to engage customers. ?? 2009 Kelley School of Business, Indiana University.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mangold, W. Glynn and Faulds, David J.},
doi = {10.1016/j.bushor.2009.03.002},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/social{\_}media{\_}promotion{\_}mix.pdf:pdf},
isbn = {0007-6813},
issn = {00076813},
journal = {Business Horizons},
keywords = {Consumer-generated media,Integrated marketing communications,Promotion mix,Social media},
number = {4},
pages = {357--365},
pmid = {41242974},
title = {{Social media: The new hybrid element of the promotion mix}},
volume = {52},
year = {2009}
}
@article{McAfee2012,
author = {McAfee, Andrew and Brynjolfsson, Erik and Davenport, Thomas H},
file = {:Users/felix/code/ml/master-thesis/literature/hbr{\_}big{\_}data.pdf:pdf},
journal = {Harvard buisness Review},
number = {10},
pages = {60--68},
title = {{Big Data : The management revolution}},
volume = {90},
year = {2012}
}
@article{McCulloch1943,
abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, W. S. and Pitts, W.},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/biological{\_}learning.pdf:pdf},
isbn = {0007-4985},
issn = {0007-4985},
journal = {Bulletin of Mathematical Biophysics},
keywords = {McCulloch and Pitts,neuron},
pages = {115--133},
pmid = {2185863},
title = {{A Logical Calculus of the Idea Immanent in Nervous Activity}},
url = {http://www.cse.chalmers.se/{~}coquand/AUTOMATA/mcp.pdf},
volume = {5},
year = {1943}
}
@book{Minsky1969,
abstract = {Perceptrons - the first systematic study of parallelism in computation - has remained a classical work on threshold automata networks for nearly two decades.},
author = {Minsky, Marvin and Papert, Seymour},
booktitle = {MIT Press},
doi = {10.1016/S0019-9958(70)90409-2},
isbn = {0262631113},
issn = {00199958},
title = {{Perceptrons}},
url = {http://mitpress.mit.edu/book-home.tcl?isbn=0262631113},
year = {1969}
}
@book{Mitchell1997,
abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. The book is intended to support upper level undergraduate and introductory level graduate courses in machine learning.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Mitchell, Tom M.},
booktitle = {McGraw-Hill},
doi = {10.1145/242224.242229},
eprint = {0-387-31073-8},
isbn = {0071154671},
issn = {9780071154673},
pages = {414},
pmid = {20236947},
title = {{Machine Learning}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0070428077},
year = {1997}
}
@article{Mnih2015,
abstract = {Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.},
archivePrefix = {arXiv},
arxivId = {1604.03986},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1604.03986},
file = {:Users/felix/code/ml/master-thesis/literature/DeepMindNature14236Paper.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236 http://arxiv.org/abs/1604.03986 http://www.nature.com/doifinder/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@inproceedings{Naveed2011,
abstract = {On the microblogging site Twitter, users can forward any message they receive to all of their followers. This is called a retweet and is usually done when users find a message particularly interesting and worth sharing with others. Thus, retweets reflect what the Twitter community considers interesting on a global scale, and can be used as a function of interestingness to generate a model to describe the content-based characteristics of retweets. In this paper, we analyze a set of high- and low-level content-based features on several large collections of Twitter messages. We train a prediction model to forecast for a given tweet its likelihood of being retweeted based on its contents. From the parameters learned by the model we de- duce what are the influential content features that contribute to the likelihood of a retweet. As a result we obtain insights into what makes a message on Twitter worth retweeting and, thus, interest- ing.},
annote = {Predict RT probability based on content features using logistic regression model

Distinguish between high-level and low-level features
Low-level (i.e. no processing needed): words, URLs, hashtags, mentions, positive/negative terms
High-level: association to topics, sentiments

Good source for content-based features

Many sources for influence research},
author = {Naveed, Nasir and Gottron, Thomas and Kunegis, J{\'{e}}r{\^{o}}me and Alhadi, Arifah Che},
booktitle = {Proceedings of the 3rd International Web Science Conference},
doi = {10.1145/2527031.2527052},
file = {:Users/felix/code/ml/master-thesis/literature/Bad{\_}News{\_}Travel{\_}Fast{\_}A{\_}Content-based{\_}Analysis{\_}of{\_}I.pdf:pdf},
isbn = {9781450308557},
number = {June},
pages = {8},
title = {{Bad news travel fast: A content-based analysis of interestingness on Twitter}},
url = {http://dl.acm.org/citation.cfm?doid=2527031.2527052},
year = {2011}
}
@article{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
journal = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2015}
}
@misc{Olah2015,
author = {Olah, Chris},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2017-01-25},
year = {2015}
}
@article{Peng2011,
abstract = {Among the most popular micro-blogging service, Twitter recently introduced their reblogging service called retweet to allow a user to repopulate another user's content for his followers. It quickly becomes one of the most prominent features on Twitter and an important mean for secondary content promotion. However, it remains unclear what motivates users to retweet and whether the retweeting decisions are predictable based on a user's tweeting history and social relationships. In this paper, we propose modeling the retweet patterns using conditional random fields with a three types of user-tweet features: content influence, network influence and temporal decay factor. We also investigate approaches to partition the social graphs and construct the network relations for retweet prediction. Our experiments demonstrate that CRF can improve prediction effectiveness by incorporating social relationships compared to the baselines that do not.},
annote = {Good definition of prediction problem

Separate three groups of features: user, relationship and tweet features

User and network features more useful for prediction than tweet features},
author = {Peng, Huan-Kai and Zhu, Jiang and Piao, Dongzhen and Yan, Rong and Zhang, Ying},
doi = {10.1109/ICDMW.2011.146},
file = {:Users/felix/code/ml/master-thesis/literature/crf{\_}retweeting{\_}dmcii2011.pdf:pdf},
isbn = {978-0-7695-4409-0},
issn = {2375-9232},
journal = {2011 IEEE 11th International Conference on Data Mining Workshops},
keywords = {Conditional Random Fields,Social Network,Twitter},
pages = {336--343},
title = {{Retweet Modeling Using Conditional Random Fields}},
year = {2011}
}
@inproceedings{Petrovic2011,
abstract = {Twitter is a very popular way for people to share informa- tion on a bewildering multitude of topics. Tweets are propa- gated using a variety of channels: by following users or lists, by searching or by retweeting. Of these vectors, retweeting is arguably the most effective, as it can potentially reach the most people, given its viral nature. A key task is predicting if a tweet will be retweeted, and solving this problem fur- thers our understanding of message propagation within large user communities. We carry out a human experiment on the task of deciding whether a tweet will be retweeted which shows that the task is possible, as human performance levels are much above chance. Using a machine learning approach based on the passive-aggressive algorithm, we are able to au- tomatically predict retweets as well as humans. Analyzing the learned model, we find that performance is dominated by so- cial features, but that tweet features add a substantial boost},
annote = {Build prediction model in realistically deployed (i.e. streaming prediction) setting

Predict binary variable, i.e. binary classification task

Compare with human experiment results

Model outperforms humans on the given task

Use passive-aggressive (PA) algorithm

Distinguish between social and tweet features
Use standard social features, but also give valuable intuitions for each feature
Use standard tweet features},
author = {Petrovic, Sasa and Osborne, Miles and Lavrenko, Victor},
booktitle = {Proceedings of the Fifth International Conference on Weblogs and Social Media},
file = {:Users/felix/code/ml/master-thesis/literature/icwsm11.pdf:pdf},
keywords = {poster papers},
pages = {586--589},
title = {{Rt to win! predicting message propagation in twitter}},
url = {http://homepages.inf.ed.ac.uk/miles/papers/icwsm11.pdf{\%}5Cnhttp://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewPDFInterstitial/2754/3209},
year = {2011}
}
@article{Polyak1964,
abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, ..., xn, ..., which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ≤ t ≤ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1we use k previous iterations xn, ..., xn-k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0. {\textcopyright} 1964.},
author = {Polyak, B. T.},
doi = {10.1016/0041-5553(64)90137-5},
file = {:Users/felix/Downloads/1964{\_}05-ucmmp.pdf:pdf},
issn = {00415553},
journal = {USSR Computational Mathematics and Mathematical Physics},
number = {5},
pages = {1--17},
title = {{Some methods of speeding up the convergence of iteration methods}},
volume = {4},
year = {1964}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:Users/felix/code/ml/master-thesis/literature/1511.06434.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--16},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@inproceedings{Raina2009,
abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton {\&} Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
booktitle = {Proceedings of the 26th International Conference on Machine Learning (ICML 09)},
doi = {10.1145/1553374.1553486},
file = {:Users/felix/code/ml/master-thesis/literature/icml09-LargeScaleUnsupervisedDeepLearningGPU.pdf:pdf},
isbn = {9781605585161},
issn = {12258687},
pages = {873--880},
pmid = {17394762},
title = {{Large-scale Deep Unsupervised Learning Using Graphics Processors}},
url = {http://dl.acm.org/citation.cfm?id=1553374.1553486},
year = {2009}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1112.6209},
author = {Rosenblatt, Frank},
doi = {10.1037/h0042519},
eprint = {arXiv:1112.6209},
file = {:Users/felix/code/ml/master-thesis/literature/rosenblatt{\_}perceptron.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471(Electronic);0033-295X(Print)},
journal = {Psychological Review},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in {\ldots}}},
url = {http://psycnet.apa.org/journals/rev/65/6/386.pdf{\%}5Cnpapers://c53d1644-cd41-40df-912d-ee195b4a4c2b/Paper/p15420},
volume = {65},
year = {1958}
}
@misc{Rumelhart1986a,
abstract = {What makes people smarter than computers? The work described in these two volumes suggests that the answer lies in the massively parallel architecture of the human mind. It is some of the most exciting work in cognitive science, unifying neural and cognitive processes in a highly computational framework, with links to artificial intelligence. Although thought and problem solving have a sequential character when viewed over a time frame of minutes or hours, the authors argue that each step in the sequence is the result of the simultaneous activity of a large number of simple computational elements, each influencing others and being influenced by them. "Parallel Distributed Processing" describes their work in developing a theoretical framework for describing this parallel distributed processing activity and in applying the framework to the development of models of aspects of perception, memory, language, and thought. Volume 1 lays the theoretical foundations of parallel distributed processing. It introduces the approach and the reasons why the authors feel it is a fruitful one, describes several models of basic mechanisms with wide applicability to different problems, and presents a number of specific technical analyses of different aspects of parallel distributed models.},
author = {Rumelhart, D. E. and Hinton, G. E. and Mcclelland, James L},
booktitle = {Parallel distributed processing: explorations in the microstructure of cognition},
isbn = {026268053x},
pages = {45 -- 76},
title = {{A General framework for Parallel Distributed Processing}},
year = {1986}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/backpropagation{\_}origins.pdf:pdf},
isbn = {0262661160},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@book{Russell1995,
abstract = {The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Russell, Stuart J. and Norvig, Peter},
booktitle = {Neurocomputing},
doi = {10.1016/0925-2312(95)90020-9},
eprint = {9809069v1},
isbn = {9780131038059},
issn = {09252312},
number = {2},
pages = {215--218},
pmid = {20949757},
primaryClass = {arXiv:gr-qc},
title = {{Artificial Intelligence: A Modern Approach}},
url = {http://portal.acm.org/citation.cfm?id=773294},
volume = {9},
year = {1995}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
file = {:Users/felix/code/ml/master-thesis/literature/goNature.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@inproceedings{Simard2003,
abstract = {Not Available},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Simard, P. Y. and Steinkraus, D. and Platt, John C.},
booktitle = {Seventh International Conference on Document Analysis and Recognition},
doi = {10.1109/ICDAR.2003.1227801},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/Simard.pdf:pdf},
isbn = {0-7695-1960-1},
issn = {15205363},
keywords = {Best practices,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural networks,Performance analysis,Support vector machines,Text analysis},
pages = {958--963},
pmid = {25246403},
title = {{Best practices for convolutional neural networks applied to visual document analysis}},
year = {2003}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/felix/code/ml/master-thesis/literature/simonyan{\_}2014{\_}vgg.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:Users/felix/code/ml/master-thesis/literature/srivastava14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Suh,
author = {Suh, Bongwon and Hong, Lichan and Pirolli, Peter and Chi, Ed H},
file = {:Users/felix/code/ml/master-thesis/literature/2010-04-15-retweetability-v18-final.pdf:pdf},
journal = {2010 IEEE Second International Conference on Social Computing (SocialCom)},
keywords = {-twitter,factor analysis,follower,information diffusion since the,namely the followers of,new set of audiences,original tweet is propagated,retweet,social media,social network,the retweeter,to a,tweet},
pages = {177--184},
title = {{Want to Be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network}},
year = {2010}
}
@inproceedings{Sutskever2013,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overﬁtting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectiﬁed linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modiﬁed deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2013.6639346},
eprint = {arXiv:1301.3605v3},
file = {:Users/felix/code/ml/master-thesis/literature/sutskever13.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
number = {2010},
pages = {8609--8613},
title = {{On the importance of initialization and momentum in deep learning}},
year = {2013}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
booktitle = {Advances in neural information processing systems},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:Users/felix/code/ml/master-thesis/literature/1409.3215.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@book{Sutton1998,
abstract = {Norton, B., {\&} Toohey, K. (Eds). (2004). Critical pedagogies and language learning. New York: Cambridge University Press. (362pp).},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press},
doi = {10.1016/S0065-230X(09)04001-9},
eprint = {1603.02199},
isbn = {0262193981},
issn = {0959-4388},
pages = {331},
pmid = {7888773},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@article{Touretzky1985,
abstract = {Pattern matching and variable binding are easily implemented in conventional computer architectures, but not necessarily in all architectures. In a distributed neural network architecture each symbol is represented by activity in many units and each unit contributes to the representation of many symbols. Manipulating symbols using this type of distributed representation is not as easy as with a local representation whore each unit denotes one symbol, but there is evidence that the distributed approach is the one chosen by nature. We describe a working implementation of a production system interpreter in a neural network using distributed representations for both symbols and rules. The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements. The success of our production system implementation goes some way towards answering a common criticism of connectionist theories: that they aren't powerful enough to do symbolic reasoning.},
author = {Touretzky, D. S. and Hinton, G. E.},
isbn = {CMU-CS-A},
journal = {Proceedings of the Ninth International Joint Conference on Artificial Intelligence},
pages = {238--243},
title = {{Symbols among the neurons: details of a connectionist inference architecture}},
volume = {1},
year = {1985}
}
@article{Travers1969,
abstract = {Arbitrarily selected individuals (N-296) in Nebraska and Boston are asked to generate acquaintance chains to a target person in Massachusetts, employ- ing "the small world method" (Milgram, 1967). Sixty-four chains reach the target person. Within this group the mean number of intermediaries be- tween starters and targets is 5.2. Boston starting chains reach the target person with fewer intermediaries than those starting in Nebraska; subpopula- tions in the Nebraska group do not differ among themselves. The funneling of chains through sociometric "stars" is noted, with 48 per cent of the chains passing through three persons before reaching the target. Applications of the method to studies of large scale social structure are discussed.},
archivePrefix = {arXiv},
arxivId = {Travers, Jeffrey {\{}{\&}{\}} Stanley Milgram. 1969. "},
author = {Travers, Jeffrey and Milgram, Stanley},
doi = {10.2307/2786545},
eprint = {Travers, Jeffrey {\{}{\&}{\}} Stanley Milgram. 1969. "},
file = {:Users/felix/code/ml/master-thesis/literature/travers1969.pdf:pdf},
isbn = {9781400841356},
issn = {0038-0431},
journal = {Sociometry},
number = {4},
pages = {425--443},
pmid = {15234481},
title = {{An Experimental Study of the Small World Problem}},
volume = {32},
year = {1969}
}
@techreport{Twitter2018,
author = {Twitter},
file = {:Users/felix/code/ml/master-thesis/literature/Q4{\_}2017{\_}Shareholder{\_}Letter.pdf:pdf},
pages = {1--21},
title = {{Twitter Q4 and Fiscal Year 2017 Shareholder Letter}},
url = {http://files.shareholder.com/downloads/AMDA-2F526X/2044306731x0x874459/8A4D1A1D-D184-4AFE-9AC1-F880C5EA06F1/Q415{\_}Shareholder{\_}Letter.pdf},
year = {2018}
}
@book{Wasserman1994,
abstract = {Covers methods for the analysis of social networks and applies them to examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wasserman, Stanley and Faust, Katherine},
doi = {10.1525/ae.1997.24.1.219},
eprint = {arXiv:1011.1669v3},
isbn = {9780521387071},
issn = {0094-0496},
pages = {825},
pmid = {52},
title = {{Social Network Analysis: Methods and Applications}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=CAm2DpIqRUIC{\&}oi=fnd{\&}pg=PR21{\&}dq=scott+and+carrington+social+network+methods{\&}ots=HuLuze3BNi{\&}sig=dRJDJvtPH2eSS1VKZoWZn92r1v0{\%}5Cnpapers2://publication/uuid/EB17CF58-99C3-4B1B-BE59-608AC77615C5},
year = {1994}
}
@misc{Widrow1960,
abstract = {The modern science of switching theory began with work by Shannon1 in 1938. The field has de-veloped rapidly since {\_}then, and at present a wealth of literature exists concerning the analysis and synthesis of logical networks which might range from simple interlock ... $\backslash$n},
author = {Widrow, B. and Hoff, M. E.},
booktitle = {1960 IRE WESCON Convention Record},
doi = {no DOI, URL correct},
isbn = {0-262-01097-6},
number = {4},
pages = {96 -- 104},
title = {{Adaptive switching circuits.}},
year = {1960}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
doi = {abs/1609.08144},
eprint = {1609.08144},
file = {:Users/felix/code/ml/master-thesis/literature/1609.08144v2.pdf:pdf},
isbn = {1471-0048 (Electronic)$\backslash$r1471-003X (Linking)},
issn = {1609.08144},
pages = {1--23},
pmid = {18319728},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@inproceedings{Zaman2010,
abstract = {We present a new methodology for predicting the spread of information in a so- cial network. We focus on the Twitter network, where information is in the form of 140 character messages called tweets, and information is spread by users for- warding tweets, a practice known as retweeting. Using data of who and what was retweeted, we train a probabilistic collaborative filter model to predict future retweets. We find that the most important features for prediction are the identity of the source of the tweet and retweeter. Our methodology is quite flexible and be used as a basis for other prediction models in social networks.},
annote = {Local prediction model based on collaborative filtering methods

Negative log-score as useful performance evaluation metric},
author = {Zaman, Tauhid R and Herbrich, Ralf and {Van Gael}, Jurgen and Stern, David},
booktitle = {Proceedings of the Computational Social Science and the Wisdom of Crowds Workshop, NIPS},
file = {:Users/felix/code/ml/master-thesis/literature/10.1.1.208.5687.pdf:pdf},
number = {45},
pages = {17599--17601},
title = {{Predicting Information Spreading in Twitter}},
url = {http://research.microsoft.com/pubs/141866/NIPS10{\_}Twitter{\_}final.pdf},
volume = {104},
year = {2010}
}
@article{Zaman2014,
abstract = {We predict the popularity of short messages called tweets created in the micro-blogging site known as Twitter. We measure the popularity of a tweet by the time-series path of its retweets, which is when people forward the tweet to others. We develop a probabilistic model for the evolution of the retweets using a Bayesian approach, and form predictions using only observations on the retweet times and the local network or "graph" structure of the retweeters. We obtain good step ahead forecasts and predictions of the final total number of retweets even when only a small fraction (i.e. less than one tenth) of the retweet paths are observed. This translates to good predictions within a few minutes of a tweet being posted and has potential implications for understanding the spread of broader ideas, memes, or trends in social networks and also revenue models for both individuals who "sell tweets" and for those looking to monetize their reach.},
annote = {Predict popularity of a tweet by predicting the time path of retweets it receives

Comparable to my problem in that they try to predict the eventual number of retweets

Small number of features: timing information, depth of retweet graph and number of followers

Tweets are at least a week old so that there are likely no more retweets occurring

Source for assumption about retweet distribution
Time to reach median of total retweet count: between four minutes and three hours
Most retweets come from the first-degree network
Log-normally distributed reaction times

Positive influence of number of followers
Retweet is less likely the farther away from the root user},
archivePrefix = {arXiv},
arxivId = {1304.6777},
author = {Zaman, Tauhid and Fox, Emily B. and Bradlow, Eric T.},
doi = {10.1214/14-AOAS741},
eprint = {1304.6777},
file = {:Users/felix/code/ml/master-thesis/literature/1304.6777.pdf:pdf},
issn = {19417330},
journal = {Annals of Applied Statistics},
keywords = {Bayesian inference,Forecasting,Social networks,Time series,Twitter},
number = {3},
pages = {1583--1611},
title = {{A bayesian approach for predicting the popularity of tweets}},
volume = {8},
year = {2014}
}
