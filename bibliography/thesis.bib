@article{Go2009,
abstract = {We introduce a novel approach for automatically classifying the sentiment of Twitter messages. These messages are classified as either positive or negative with respect to a query term. This is useful for consumers who want to research the sentiment of products before purchase, or companies that want to monitor the public sentiment of their brands. There is no previous research on classifying sentiment of messages on microblogging services like Twitter. We present the results of machine learning algorithms for classifying the sentiment of Twitter messages using distant supervision. Our training data consists of Twitter messages with emoticons, which are used as noisy labels. This type of training data is abundantly available and can be obtained through automated means. We show that machine learning algorithms (Naive Bayes, Maximum Entropy, and SVM) have accuracy above 80{\%} when trained with emoticon data. This paper also describes the preprocessing steps needed in order to achieve high accuracy. The main contribution of this paper is the idea of using tweets with emoticons for distant supervised learning.},
archivePrefix = {arXiv},
arxivId = {1702.08388},
author = {Go, Alec and Bhayani, Richa and Huang, Lei},
doi = {10.1016/j.sedgeo.2006.07.004},
eprint = {1702.08388},
isbn = {1012341234},
issn = {00370738},
journal = {CS224N Project Report, Stanford},
keywords = {sentiment analysis,sentiment classification,twitter},
number = {12},
pages = {12},
pmid = {791643259},
title = {{Twitter sentiment classification using distant supervision}},
url = {http://www.stanford.edu/{~}alecmgo/papers/TwitterDistantSupervision09.pdf},
volume = {1},
year = {2009}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic tech..., (展开全部)},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M.},
booktitle = {Springer},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition And Machine Learning}},
url = {http://book.douban.com/subject/2061116/},
year = {2006}
}
@book{Wasserman1994,
abstract = {Covers methods for the analysis of social networks and applies them to examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wasserman, Stanley and Faust, Katherine},
doi = {10.1525/ae.1997.24.1.219},
eprint = {arXiv:1011.1669v3},
isbn = {9780521387071},
issn = {0094-0496},
pages = {825},
pmid = {52},
title = {{Social Network Analysis: Methods and Applications}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=CAm2DpIqRUIC{\&}oi=fnd{\&}pg=PR21{\&}dq=scott+and+carrington+social+network+methods{\&}ots=HuLuze3BNi{\&}sig=dRJDJvtPH2eSS1VKZoWZn92r1v0{\%}5Cnpapers2://publication/uuid/EB17CF58-99C3-4B1B-BE59-608AC77615C5},
year = {1994}
}
@article{Barabasi2003,
abstract = {Scientists have recently discovered that various complex systems have an underlying architecture governed by shared organizing principles. This insight has important implications for a host of applications, from drug development to Internet security},
author = {Barab{\'{a}}si, Albert-l{\'{a}}szl{\'{o}} and Bonabeau, E},
doi = {10.4249/scholarpedia.1716},
file = {:Users/felix/code/ml/master-thesis/literature/barabasisciam.pdf:pdf},
isbn = {9780874807554},
issn = {00368733},
journal = {Scientific American},
number = {1},
pages = {50--59},
pmid = {12701331},
title = {{Scale-free networks}},
url = {http://eaton.math.rpi.edu/csums/papers/FoodWebs/barabasisciam.pdf{\%}5Cnhttp://www.scholarpedia.org/article/Scale-free{\_}networks{\%}5Cnhttp://link.springer.com/article/10.1007/s11576-006-0058-2},
volume = {3},
year = {2003}
}
@misc{Granovetter1973,
abstract = {Granovetter, S. (1973) The strength of weak ties, American Journal of Sociology, 78(6), pp. 1360–1380.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Granovetter, Mark},
booktitle = {The American Journal of Sociology},
doi = {10.1086/225469},
eprint = {NIHMS150003},
file = {:Users/felix/code/ml/master-thesis/literature/the{\_}strength{\_}of{\_}weak{\_}ties{\_}and{\_}exch{\_}w-gans.pdf:pdf},
isbn = {9783540723462},
issn = {0002-9602},
number = {6},
pages = {1360--1380},
pmid = {19848562},
title = {{The Strength of Weak Ties}},
url = {https://sociology.stanford.edu/sites/default/files/publications/the{\_}strength{\_}of{\_}weak{\_}ties{\_}and{\_}exch{\_}w-gans.pdf},
volume = {78},
year = {1973}
}
@techreport{Twitter2018,
author = {Twitter},
file = {:Users/felix/code/ml/master-thesis/literature/Q4{\_}2017{\_}Shareholder{\_}Letter.pdf:pdf},
pages = {1--21},
title = {{Twitter Q4 and Fiscal Year 2017 Shareholder Letter}},
url = {http://files.shareholder.com/downloads/AMDA-2F526X/2044306731x0x874459/8A4D1A1D-D184-4AFE-9AC1-F880C5EA06F1/Q415{\_}Shareholder{\_}Letter.pdf},
year = {2018}
}
@article{Travers1969,
abstract = {Arbitrarily selected individuals (N-296) in Nebraska and Boston are asked to generate acquaintance chains to a target person in Massachusetts, employ- ing "the small world method" (Milgram, 1967). Sixty-four chains reach the target person. Within this group the mean number of intermediaries be- tween starters and targets is 5.2. Boston starting chains reach the target person with fewer intermediaries than those starting in Nebraska; subpopula- tions in the Nebraska group do not differ among themselves. The funneling of chains through sociometric "stars" is noted, with 48 per cent of the chains passing through three persons before reaching the target. Applications of the method to studies of large scale social structure are discussed.},
archivePrefix = {arXiv},
arxivId = {Travers, Jeffrey {\{}{\&}{\}} Stanley Milgram. 1969. "},
author = {Travers, Jeffrey and Milgram, Stanley},
doi = {10.2307/2786545},
eprint = {Travers, Jeffrey {\{}{\&}{\}} Stanley Milgram. 1969. "},
file = {:Users/felix/code/ml/master-thesis/literature/travers1969.pdf:pdf},
isbn = {9781400841356},
issn = {0038-0431},
journal = {Sociometry},
number = {4},
pages = {425--443},
pmid = {15234481},
title = {{An Experimental Study of the Small World Problem}},
volume = {32},
year = {1969}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
doi = {10.1167/16.12.326},
eprint = {1508.06576},
file = {:Users/felix/code/ml/master-thesis/literature/1508.06576.pdf:pdf},
isbn = {2200000006},
issn = {1534-7362},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
url = {http://arxiv.org/abs/1508.06576},
year = {2015}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:Users/felix/code/ml/master-thesis/literature/1511.06434.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--16},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
file = {:Users/felix/code/ml/master-thesis/literature/goNature.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Mnih2015,
abstract = {Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.},
archivePrefix = {arXiv},
arxivId = {1604.03986},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1604.03986},
file = {:Users/felix/code/ml/master-thesis/literature/DeepMindNature14236Paper.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236 http://arxiv.org/abs/1604.03986 http://www.nature.com/doifinder/10.1038/nature14236},
volume = {518},
year = {2015}
}
@inproceedings{Girshick2012,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:Users/felix/code/ml/master-thesis/literature/Girshick{\_}Rich{\_}Feature{\_}Hierarchies{\_}2014{\_}CVPR{\_}paper.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
month = {nov},
pages = {580--587},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@inproceedings{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
booktitle = {International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.322},
eprint = {1703.06870},
file = {:Users/felix/code/ml/master-thesis/literature/1703.06870.pdf:pdf},
isbn = {978-1-5386-1032-9},
issn = {0006-291X},
pages = {2980--2988},
pmid = {303902},
title = {{Mask R-CNN}},
url = {http://arxiv.org/abs/1703.06870},
year = {2017}
}
@article{Karpathy2017,
abstract = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Fei-Fei, Li},
doi = {10.1109/TPAMI.2016.2598339},
eprint = {1412.2306},
file = {:Users/felix/code/ml/master-thesis/literature/cvpr2015.pdf:pdf},
isbn = {9781467369640},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Image captioning,deep neural networks,language model,recurrent neural network,visual-semantic embeddings},
number = {4},
pages = {664--676},
pmid = {16873662},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
volume = {39},
year = {2017}
}
@article{Bertinetto2016,
abstract = {The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.},
archivePrefix = {arXiv},
arxivId = {1606.09549},
author = {Bertinetto, Luca and Valmadre, Jack and Henriques, Jo{\~{a}}o F. and Vedaldi, Andrea and Torr, Philip H. S.},
doi = {10.1007/978-3-319-48881-3_56},
eprint = {1606.09549},
file = {:Users/felix/code/ml/master-thesis/literature/1606.09549.pdf:pdf},
isbn = {9783319488806},
issn = {16113349},
journal = {European conference on computer vision},
keywords = {Deep-learning,Object-tracking,Siamese-network,Similarity-learning},
month = {jun},
number = {October},
pages = {850--865},
pmid = {4520227},
title = {{Fully-Convolutional Siamese Networks for Object Tracking}},
url = {http://arxiv.org/abs/1606.09549},
year = {2016}
}
@inproceedings{Kim2015,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
booktitle = {AAAI},
doi = {2},
eprint = {1508.06615},
file = {:Users/felix/code/ml/master-thesis/literature/1508.06615.pdf:pdf},
isbn = {9781577357605},
issn = {14814374},
pages = {2741--2749},
pmid = {15003161},
title = {{Character-Aware Neural Language Models}},
url = {http://arxiv.org/abs/1508.06615},
year = {2016}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
doi = {abs/1609.08144},
eprint = {1609.08144},
file = {:Users/felix/code/ml/master-thesis/literature/1609.08144v2.pdf:pdf},
isbn = {1471-0048 (Electronic)$\backslash$r1471-003X (Linking)},
issn = {1609.08144},
pages = {1--23},
pmid = {18319728},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{Conneau2016,
abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.},
archivePrefix = {arXiv},
arxivId = {1606.01781},
author = {Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"{i}}c and Lecun, Yann},
doi = {10.1007/s13218-012-0198-z},
eprint = {1606.01781},
file = {:Users/felix/code/ml/master-thesis/literature/1606.01781.pdf:pdf},
isbn = {9789814618038},
issn = {0933-1875},
journal = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
pages = {1107--1116},
pmid = {10463930},
title = {{Very Deep Convolutional Networks for Text Classification}},
url = {http://arxiv.org/abs/1606.01781},
volume = {1},
year = {2017}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
booktitle = {Advances in neural information processing systems},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:Users/felix/code/ml/master-thesis/literature/1409.3215.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@article{Jordan2015,
author = {Jordan, M. I. and Mitchell, T. M.},
file = {:Users/felix/code/ml/master-thesis/literature/jordan{\_}mitchell{\_}2015{\_}machine{\_}learning.pdf:pdf},
journal = {Science},
number = {6245},
pages = {255--260},
title = {{Machine learning: Trends, perspectives, and prospects}},
volume = {349},
year = {2015}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/felix/code/ml/master-thesis/literature/1406.1078v3.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@inproceedings{Raina2009,
abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton {\&} Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
booktitle = {Proceedings of the 26th International Conference on Machine Learning (ICML 09)},
doi = {10.1145/1553374.1553486},
file = {:Users/felix/code/ml/master-thesis/literature/icml09-LargeScaleUnsupervisedDeepLearningGPU.pdf:pdf},
isbn = {9781605585161},
issn = {12258687},
pages = {873--880},
pmid = {17394762},
title = {{Large-scale Deep Unsupervised Learning Using Graphics Processors}},
url = {http://dl.acm.org/citation.cfm?id=1553374.1553486},
year = {2009}
}
@article{McAfee2012,
author = {McAfee, Andrew and Brynjolfsson, Erik and Davenport, Thomas H},
file = {:Users/felix/code/ml/master-thesis/literature/hbr{\_}big{\_}data.pdf:pdf},
journal = {Harvard buisness Review},
number = {10},
pages = {60--68},
title = {{Big Data : The management revolution}},
volume = {90},
year = {2012}
}
@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
doi = {10.1038/nn.3331},
eprint = {1603.04467},
file = {:Users/felix/code/ml/master-thesis/literature/1603.04467.pdf:pdf},
isbn = {0010-0277},
issn = {0270-6474},
journal = {OSDI},
pages = {265--283},
pmid = {16411492},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
volume = {16},
year = {2016}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:Users/felix/code/ml/master-thesis/literature/srivastava14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@inproceedings{Sutskever2013,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overﬁtting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectiﬁed linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modiﬁed deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2013.6639346},
eprint = {arXiv:1301.3605v3},
file = {:Users/felix/code/ml/master-thesis/literature/sutskever13.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
number = {2010},
pages = {8609--8613},
title = {{On the importance of initialization and momentum in deep learning}},
year = {2013}
}
@article{Kingma2014a,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:Users/felix/code/ml/master-thesis/literature/1412.6980.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
pages = {1--15},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1017/CBO9781139058452},
eprint = {arXiv:1406.2661v1},
file = {:Users/felix/code/ml/master-thesis/literature/5423-generative-adversarial-nets.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
pmid = {1000183096},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@inproceedings{Simard2003,
abstract = {Not Available},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Simard, P. Y. and Steinkraus, D. and Platt, John C.},
booktitle = {Seventh International Conference on Document Analysis and Recognition},
doi = {10.1109/ICDAR.2003.1227801},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/Simard.pdf:pdf},
isbn = {0-7695-1960-1},
issn = {15205363},
keywords = {Best practices,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural networks,Performance analysis,Support vector machines,Text analysis},
pages = {958--963},
pmid = {25246403},
title = {{Best practices for convolutional neural networks applied to visual document analysis}},
year = {2003}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/felix/code/ml/master-thesis/literature/simonyan{\_}2014{\_}vgg.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Hinton2012a,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
doi = {arXiv:1207.0580},
eprint = {1207.0580},
file = {:Users/felix/code/ml/master-thesis/literature/1207.0580.pdf:pdf},
isbn = {9781467394673},
issn = {9781467394673},
pages = {1--18},
pmid = {1000104337},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@misc{Olah2015,
author = {Olah, Chris},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2017-01-25},
year = {2015}
}
@article{Polyak1964,
abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, ..., xn, ..., which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ≤ t ≤ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1we use k previous iterations xn, ..., xn-k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0. {\textcopyright} 1964.},
author = {Polyak, B. T.},
doi = {10.1016/0041-5553(64)90137-5},
file = {:Users/felix/Downloads/1964{\_}05-ucmmp.pdf:pdf},
issn = {00415553},
journal = {USSR Computational Mathematics and Mathematical Physics},
number = {5},
pages = {1--17},
title = {{Some methods of speeding up the convergence of iteration methods}},
volume = {4},
year = {1964}
}
@inproceedings{Ioffe2015,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {International conference on machine learning},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/Downloads/1502.03167.pdf:pdf},
isbn = {9780874216561},
issn = {10282092},
keywords = {Blechnaceae,Indonesia,New species,Stenochlaena},
pages = {448--456},
pmid = {15003161},
title = {{Batch Normalization}},
url = {https://arxiv.org/pdf/1502.03167.pdf},
year = {2015}
}
@inproceedings{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/felix/Downloads/1998Lecun.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@misc{Rumelhart1986a,
abstract = {What makes people smarter than computers? The work described in these two volumes suggests that the answer lies in the massively parallel architecture of the human mind. It is some of the most exciting work in cognitive science, unifying neural and cognitive processes in a highly computational framework, with links to artificial intelligence. Although thought and problem solving have a sequential character when viewed over a time frame of minutes or hours, the authors argue that each step in the sequence is the result of the simultaneous activity of a large number of simple computational elements, each influencing others and being influenced by them. "Parallel Distributed Processing" describes their work in developing a theoretical framework for describing this parallel distributed processing activity and in applying the framework to the development of models of aspects of perception, memory, language, and thought. Volume 1 lays the theoretical foundations of parallel distributed processing. It introduces the approach and the reasons why the authors feel it is a fruitful one, describes several models of basic mechanisms with wide applicability to different problems, and presents a number of specific technical analyses of different aspects of parallel distributed models.},
author = {Rumelhart, D. E. and Hinton, G. E. and Mcclelland, James L},
booktitle = {Parallel distributed processing: explorations in the microstructure of cognition},
isbn = {026268053x},
pages = {45 -- 76},
title = {{A General framework for Parallel Distributed Processing}},
year = {1986}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
eprint = {arXiv:1011.1669v3},
isbn = {0340-1200},
issn = {03401200},
journal = {Biological Cybernetics},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@book{Russell1995,
abstract = {The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Russell, Stuart J. and Norvig, Peter},
booktitle = {Neurocomputing},
doi = {10.1016/0925-2312(95)90020-9},
eprint = {9809069v1},
isbn = {9780131038059},
issn = {09252312},
number = {2},
pages = {215--218},
pmid = {20949757},
primaryClass = {arXiv:gr-qc},
title = {{Artificial Intelligence: A Modern Approach}},
url = {http://portal.acm.org/citation.cfm?id=773294},
volume = {9},
year = {1995}
}
@book{Mitchell1997,
abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. The book is intended to support upper level undergraduate and introductory level graduate courses in machine learning.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Mitchell, Tom M.},
booktitle = {McGraw-Hill},
doi = {10.1145/242224.242229},
eprint = {0-387-31073-8},
isbn = {0071154671},
issn = {9780071154673},
pages = {414},
pmid = {20236947},
title = {{Machine Learning}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0070428077},
year = {1997}
}
@article{Hinton2012,
abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel Rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
doi = {10.1109/MSP.2012.2205597},
eprint = {1207.0580},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {82--97},
pmid = {13057166},
title = {{Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups}},
volume = {29},
year = {2012}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it b...},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, Jurgen J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:Users/felix/Downloads/Bobby{\_}paper1.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long short-term memory}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735{\%}5Cnfile:///Files/F0/F0267A41-D807-4137-A5AC-8D9A84E9E12C.pdf{\%}5Cnpapers3://publication/doi/10.1162/neco.1997.9.8.1735{\%}5Cnfile:///Files/B1/B10E2649-D486-4D93-B71B-80023681156B.pdf},
volume = {9},
year = {1997}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Brynjolfsson2017,
abstract = {For more than 250 years the fundamental drivers of economic growth have been technological innovations. The most important of these are what economists call general-purpose technologies — a category that includes the steam engine, electricity, and the internal combustion engine. Each one catalyzed waves of complementary innovations and opportunities. The internal combustion engine, for example, gave rise to cars, trucks, airplanes, chain saws, and lawnmowers, along with big-box retailers, shopping centers, crossdocking warehouses, new supply chains, and, when you think about it, suburbs. Companies as diverse as Walmart, UPS, and Uber found ways to leverage the technology to create profitable new business models.},
author = {Brynjolfsson, Erik and Mcafee, Andrew},
journal = {Harvard Buisness Review},
pages = {1--8},
title = {{The Business of Artificial Intelligence}},
url = {https://hbr.org/cover-story/2017/07/the-business-of-artificial-intelligence},
year = {2017}
}
@article{Hahnioser2000,
abstract = {Digital circuits such as the flip-flop use feedback to achieve multistability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
author = {Hahnioser, Richard H.R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
doi = {10.1038/35016072},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {00280836},
journal = {Nature},
number = {6789},
pages = {947--951},
pmid = {10879535},
title = {{Digital selection and analogue amplification coexist in a cortex- inspired silicon circuit}},
volume = {405},
year = {2000}
}
@book{Minsky1969,
abstract = {Perceptrons - the first systematic study of parallelism in computation - has remained a classical work on threshold automata networks for nearly two decades.},
author = {Minsky, Marvin and Papert, Seymour},
booktitle = {MIT Press},
doi = {10.1016/S0019-9958(70)90409-2},
isbn = {0262631113},
issn = {00199958},
title = {{Perceptrons}},
url = {http://mitpress.mit.edu/book-home.tcl?isbn=0262631113},
year = {1969}
}
@book{Sutton1998,
abstract = {Norton, B., {\&} Toohey, K. (Eds). (2004). Critical pedagogies and language learning. New York: Cambridge University Press. (362pp).},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press},
doi = {10.1016/S0065-230X(09)04001-9},
eprint = {1603.02199},
isbn = {0262193981},
issn = {0959-4388},
pages = {331},
pmid = {7888773},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@misc{Widrow1960,
abstract = {The modern science of switching theory began with work by Shannon1 in 1938. The field has de-veloped rapidly since {\_}then, and at present a wealth of literature exists concerning the analysis and synthesis of logical networks which might range from simple interlock ... $\backslash$n},
author = {Widrow, B. and Hoff, M. E.},
booktitle = {1960 IRE WESCON Convention Record},
doi = {no DOI, URL correct},
isbn = {0-262-01097-6},
number = {4},
pages = {96 -- 104},
title = {{Adaptive switching circuits.}},
year = {1960}
}
@article{Touretzky1985,
abstract = {Pattern matching and variable binding are easily implemented in conventional computer architectures, but not necessarily in all architectures. In a distributed neural network architecture each symbol is represented by activity in many units and each unit contributes to the representation of many symbols. Manipulating symbols using this type of distributed representation is not as easy as with a local representation whore each unit denotes one symbol, but there is evidence that the distributed approach is the one chosen by nature. We describe a working implementation of a production system interpreter in a neural network using distributed representations for both symbols and rules. The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements. The success of our production system implementation goes some way towards answering a common criticism of connectionist theories: that they aren't powerful enough to do symbolic reasoning.},
author = {Touretzky, D. S. and Hinton, G. E.},
isbn = {CMU-CS-A},
journal = {Proceedings of the Ninth International Joint Conference on Artificial Intelligence},
pages = {238--243},
title = {{Symbols among the neurons: details of a connectionist inference architecture}},
volume = {1},
year = {1985}
}
@article{Fukushima1975,
abstract = {A new hypothesis for the organization of synapses between neurons is proposed: ``The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y''. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organised can be deduced. A self-organising multilayered neural network, which is named ``cognitron'', is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favourable without having a ``teacher'' which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00342633},
issn = {03401200},
journal = {Biological Cybernetics},
number = {3-4},
pages = {121--136},
pmid = {1203338},
title = {{Cognitron: A self-organizing multilayered neural network}},
volume = {20},
year = {1975}
}
@article{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
journal = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2015}
}
@book{Goodfellow2016,
abstract = {Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {MIT Press},
doi = {10.1038/nmeth.3707},
eprint = {arXiv:1312.6184v5},
file = {:Users/felix/Google Drive/04{\_}Ressources/Books/Deep Learning Book/Deep{\_}Learning{\_}Book.pdf:pdf},
isbn = {978-0262035613},
issn = {0028-0836},
pages = {800},
pmid = {26017442},
title = {{Deep Learning}},
url = {http://goodfeli.github.io/dlbook/{\%}0Ahttp://dx.doi.org/10.1038/nature14539},
volume = {1},
year = {2016}
}
@article{Mangold2009,
abstract = {The emergence of Internet-based social media has made it possible for one person to communicate with hundreds or even thousands of other people about products and the companies that provide them. Thus, the impact of consumer-to-consumer communications has been greatly magnified in the marketplace. This article argues that social media is a hybrid element of the promotion mix because in a traditional sense it enables companies to talk to their customers, while in a nontraditional sense it enables customers to talk directly to one another. The content, timing, and frequency of the social media-based conversations occurring between consumers are outside managers' direct control. This stands in contrast to the traditional integrated marketing communications paradigm whereby a high degree of control is present. Therefore, managers must learn to shape consumer discussions in a manner that is consistent with the organization's mission and performance goals. Methods by which this can be accomplished are delineated herein. They include providing consumers with networking platforms, and using blogs, social media tools, and promotional tools to engage customers. ?? 2009 Kelley School of Business, Indiana University.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mangold, W. Glynn and Faulds, David J.},
doi = {10.1016/j.bushor.2009.03.002},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/social{\_}media{\_}promotion{\_}mix.pdf:pdf},
isbn = {0007-6813},
issn = {00076813},
journal = {Business Horizons},
keywords = {Consumer-generated media,Integrated marketing communications,Promotion mix,Social media},
number = {4},
pages = {357--365},
pmid = {41242974},
title = {{Social media: The new hybrid element of the promotion mix}},
volume = {52},
year = {2009}
}
@article{Hoffman2010,
abstract = {You can. But it requires a new set of measurements that begins with tracking the customers investments not yours.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hoffman, Dl D.L. Donna L and Fodor, Marek},
doi = {10.1287/mksc.1120.0768},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/social{\_}media{\_}marketing{\_}roi.pdf:pdf},
isbn = {15329194},
issn = {15329194},
journal = {MIT Sloan Management Review},
number = {1},
pages = {41--49},
pmid = {78434266},
title = {{Can You Measure the ROI of Your Social Media Marketing?}},
url = {http://www.mitsmr-ezine.com/mitsmriphone11/fall2010/m2/MobileArticle.action?articleId=23732{\&}mobileWeb=true{\&}lm=1285614348000{\%}5Cnhttp://www.emarketingtravel.net/resources/can you mesur the ROI of your Social media marketing.pdf{\%}5Cnhttp://sloanreview},
volume = {52},
year = {2010}
}
@article{Kim2012,
abstract = {In light of a growing interest in the use of social media marketing (SMM) among luxury fashion brands, this study set out to identify attributes of SMM activities and examine the relationships among those perceived activities, value equity, relationship equity, brand equity, customer equity, and purchase intention through a structural equation model. Five constructs of perceived SSM activities of luxury fashion brands are entertainment, interaction, trendiness, customization, and word of mouth. Their effects on value equity, relationship equity, and brand equity are significantly positive. For the relationship between customer equity drivers and customer equity, brand equity has significant negative effect on customer equity while value equity and relationship equity show no significant effect. As for purchase intention, value equity and relationship equity had significant positive effects, while relationship equity had no significant influence. Finally, the relationship between purchase intention and customer equity has significance. The findings of this study can enable luxury brands to forecast the future purchasing behavior of their customers more accurately and provide a guide to managing their assets and marketing activities as well. {\textcopyright} 2011 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kim, Angella J. and Ko, Eunju},
doi = {10.1016/j.jbusres.2011.10.014},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/social{\_}media{\_}customer{\_}equity.pdf:pdf},
isbn = {0148-2963},
issn = {01482963},
journal = {Journal of Business Research},
keywords = {Brand equity,Customer equity,Luxury brands,Perceived social media marketing (SMM) activities,Purchase intention,Relationship equity,Value equity},
number = {10},
pages = {1480--1486},
pmid = {78434266},
publisher = {Elsevier Inc.},
title = {{Do social media marketing activities enhance customer equity? An empirical study of luxury fashion brand}},
url = {http://dx.doi.org/10.1016/j.jbusres.2011.10.014},
volume = {65},
year = {2012}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1112.6209},
author = {Rosenblatt, Frank},
doi = {10.1037/h0042519},
eprint = {arXiv:1112.6209},
file = {:Users/felix/code/ml/master-thesis/literature/rosenblatt{\_}perceptron.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471(Electronic);0033-295X(Print)},
journal = {Psychological Review},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in {\ldots}}},
url = {http://psycnet.apa.org/journals/rev/65/6/386.pdf{\%}5Cnpapers://c53d1644-cd41-40df-912d-ee195b4a4c2b/Paper/p15420},
volume = {65},
year = {1958}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/backpropagation{\_}origins.pdf:pdf},
isbn = {0262661160},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{McCulloch1943,
abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, W. S. and Pitts, W.},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/code/ml/master-thesis/literature/biological{\_}learning.pdf:pdf},
isbn = {0007-4985},
issn = {0007-4985},
journal = {Bulletin of Mathematical Biophysics},
keywords = {McCulloch and Pitts,neuron},
pages = {115--133},
pmid = {2185863},
title = {{A Logical Calculus of the Idea Immanent in Nervous Activity}},
url = {http://www.cse.chalmers.se/{~}coquand/AUTOMATA/mcp.pdf},
volume = {5},
year = {1943}
}
@article{Culnan2010,
author = {Culnan, Mary J. and McHugh, Patrick and Zubillaga, Jesus I.},
file = {:Users/felix/code/ml/master-thesis/literature/social{\_}media{\_}business{\_}value.pdf:pdf},
journal = {MIS Quarterly Executive},
number = {4},
pages = {243--259},
title = {{How large US companies can use Twitter and other social media to gain business value}},
volume = {9},
year = {2010}
}
@article{Kaplan2010,
abstract = {The concept of Social Media is top of the agenda for many business executives today. Decision makers, as well as consultants, try to identify ways in which firms can make profitable use of applications such as Wikipedia, YouTube, Facebook, Second Life, and Twitter. Yet despite this interest, there seems to be very limited understanding of what the term "Social Media" exactly means; this article intends to provide some clarification. We begin by describing the concept of Social Media, and discuss how it differs from related concepts such as Web 2.0 and User Generated Content. Based on this definition, we then provide a classification of Social Media which groups applications currently subsumed under the generalized term into more specific categories by characteristic: collaborative projects, blogs, content communities, social networking sites, virtual game worlds, and virtual social worlds. Finally, we present 10 pieces of advice for companies which decide to utilize Social Media. ?? 2009 Kelley School of Business, Indiana University.},
archivePrefix = {arXiv},
arxivId = {1501.00994},
author = {Kaplan, Andreas M. and Haenlein, Michael},
doi = {10.1016/j.bushor.2009.09.003},
eprint = {1501.00994},
file = {:Users/felix/code/ml/master-thesis/literature/social{\_}media{\_}challenges{\_}opportunities.pdf:pdf},
isbn = {0007-6813},
issn = {00076813},
journal = {Business Horizons},
keywords = {Social Media,Social networking sites,User Generated Content,Virtual worlds,Web 2.0},
number = {1},
pages = {59--68},
pmid = {45641953},
title = {{Users of the world, unite! The challenges and opportunities of Social Media}},
volume = {53},
year = {2010}
}
@inproceedings{Naveed2011,
abstract = {On the microblogging site Twitter, users can forward any message they receive to all of their followers. This is called a retweet and is usually done when users find a message particularly interesting and worth sharing with others. Thus, retweets reflect what the Twitter community considers interesting on a global scale, and can be used as a function of interestingness to generate a model to describe the content-based characteristics of retweets. In this paper, we analyze a set of high- and low-level content-based features on several large collections of Twitter messages. We train a prediction model to forecast for a given tweet its likelihood of being retweeted based on its contents. From the parameters learned by the model we de- duce what are the influential content features that contribute to the likelihood of a retweet. As a result we obtain insights into what makes a message on Twitter worth retweeting and, thus, interest- ing.},
annote = {Predict RT probability based on content features using logistic regression model

Distinguish between high-level and low-level features
Low-level (i.e. no processing needed): words, URLs, hashtags, mentions, positive/negative terms
High-level: association to topics, sentiments

Good source for content-based features

Many sources for influence research},
author = {Naveed, Nasir and Gottron, Thomas and Kunegis, J{\'{e}}r{\^{o}}me and Alhadi, Arifah Che},
booktitle = {Proceedings of the 3rd International Web Science Conference},
doi = {10.1145/2527031.2527052},
file = {:Users/felix/code/ml/master-thesis/literature/Bad{\_}News{\_}Travel{\_}Fast{\_}A{\_}Content-based{\_}Analysis{\_}of{\_}I.pdf:pdf},
isbn = {9781450308557},
number = {June},
pages = {8},
title = {{Bad news travel fast: A content-based analysis of interestingness on Twitter}},
url = {http://dl.acm.org/citation.cfm?doid=2527031.2527052},
year = {2011}
}
@article{Bakshy2011,
abstract = {In this paper we investigate the attributes and relative influ- ence of 1.6M Twitter users by tracking 74 million diffusion events that took place on the Twitter follower graph over a two month interval in 2009. Unsurprisingly, we find that the largest cascades tend to be generated by users who have been influential in the past and who have a large number of followers. We also find that URLs that were rated more interesting and/or elicited more positive feelings by workers on Mechanical Turk were more likely to spread. In spite of these intuitive results, however, we find that predictions of which particular user or URL will generate large cascades are relatively unreliable. We conclude, therefore, that word- of-mouth diffusion can only be harnessed reliably by tar- geting large numbers of potential influencers, thereby cap- turing average effects. Finally, we consider a family of hy- pothetical marketing strategies, defined by the relative cost of identifying versus compensating potential “influencers.” We find that although under some circumstances, the most influential users are also the most cost-effective, under a wide range of plausible assumptions the most cost-effective performance can be realized using “ordinary influencers”— individuals who exert average or even less-than-average in- fluence.},
annote = {Measure influence in social networks quantitatively

Identify influencers and developing marketing strategies

Study diffusion events in Twitter

Simple features: number of followers, friends and tweets + date of joining},
archivePrefix = {arXiv},
arxivId = {1111.1896},
author = {Bakshy, Eytan and Hofman, Jake M. and Mason, Winter A. and Watts, Duncan J.},
doi = {10.1145/1935826.1935845},
eprint = {1111.1896},
file = {:Users/felix/code/ml/master-thesis/literature/Everyones{\_}an{\_}Influencer{\_}Quantifying{\_}Influence{\_}on{\_}.pdf:pdf},
isbn = {9781450304931},
issn = {03638111},
journal = {Proceedings of the fourth ACM international conference on Web search and data mining},
keywords = {communication networks,di ff usion,influence,twitter,word},
number = {January},
pages = {65--74},
title = {{Everyone's an influencer: quantifying influence on Twitter}},
url = {http://portal.acm.org/citation.cfm?doid=1935826.1935845},
year = {2011}
}
@inproceedings{Golder,
annote = {Examines conversational aspects of retweeting

Retweeting is a behavioral convention for Twitter like mentions (directed messages) and hashtags (topical keywords)

Retweeting: form of information diffusion and participating in conversations without contributing

Motivations for RT: spread to new audiences, entertain specific audience, comment or begin conversation, add new content, show presence as listener, agree or validate w/ tweet, request of friendship or loyalty, promote less visible content, self-gain like new followers, save tweets for future access

Retweeted content: time-sensitive material, e.g. breaking news},
author = {Boyd, Danah and Golder, Scott and Lotan, Gilad},
booktitle = {2010 43rd Hawaii International Conference on System Sciences},
doi = {10.1109/HICSS.2010.412},
file = {:Users/felix/code/ml/master-thesis/literature/tweettweetretweet.pdf:pdf},
isbn = {978-1-4244-5509-6},
month = {jan},
pages = {1--10},
publisher = {IEEE},
title = {{Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter}},
url = {http://ieeexplore.ieee.org/document/5428313/},
year = {2010}
}
@inproceedings{Hong2011,
abstract = {Social network services have become a viable source of information for users. In Twitter, information deemed important by the community propagates through retweets. Studying the characteristics of such popular messages is important for a number of tasks, such as breaking news detection, personalized message recommendation, viral marketing and others. This paper investigates the problem of predicting the popularity of messages as measured by the number of future retweets and sheds some light on what kinds of factors influence information propagation in Twitter. We formulate the task into a classification problem and study two of its variants by investigating a wide spectrum of features based on the content of the messages, temporal information, metadata of messages and users, as well as structural properties of the users' social graph on a large scale dataset. We show that our method can successfully predict messages which will attract thousands of retweets with good performance.},
annote = {Build two models for predicting retweets:
First, binary model
Second, multi-class classifier (0, {\textless}100, {\textless}10000, {\textgreater}10000)

Use advanced topological features like PageRank, degree distribution},
author = {Hong, Liangjie and Dan, Ovidiu and Davison, Brian D.},
booktitle = {Proceedings of the 20th international conference companion on World wide web},
doi = {10.1145/1963192.1963222},
file = {:Users/felix/code/ml/master-thesis/literature/p57.pdf:pdf},
isbn = {9781450306379},
issn = {9781450306379},
keywords = {2,classification,features,information diffusion,microblogs,of messages,of predicting the popularity,problems,social media,we cast the problem},
pages = {57--58},
title = {{Predicting popular messages in Twitter}},
url = {http://portal.acm.org/citation.cfm?doid=1963192.1963222},
year = {2011}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
annote = {Authors examine performance increases through deepening the model

They argue that decreasing performance for deeper models (called degradation) is not caused by overfitting

Introduction of Deep Residual Networks which consist of so-called residual blocks

Residual blocks include shortcut connection where the layer input is passed through and then pair-wise combined with the outputs of several layers (e.g. convolutions)

This enables training competitive models with a depth of up to 1,000 layers without the use of regularization methods (e.g. batch normalization, dropout)

Result: deep, thin models that set new state-of-the-art on many commonly used benchmark data sets (e.g. ImageNet)},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
doi = {10.1007/s11042-017-4440-4},
eprint = {1512.03385},
file = {:Users/felix/code/ml/master-thesis/literature/1512.03385.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {15737721},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
month = {dec},
pages = {770--778},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2016}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
annote = {Description of CNN trained on the ImageNet data set

Achieves new state-of-the-art results in the ImageNet competition

Deep neural network using common methods such as dropout and data augmentation in order to avoid overfitting

Very good write-up of the model, very comprehensible},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances In Neural Information Processing Systems},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/felix/code/ml/master-thesis/literature/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {1097--1105},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Holden2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural$\backslash$rnetwork with a small central layer to reconstruct high-dimensional input vectors. Gradient descent$\backslash$r$\backslash$ncan be used for fine-tuning the weights in such ‘‘autoencoder'' networks, but this works well only if$\backslash$r$\backslash$nthe initial weights are close to a good solution. We describe an effective way of initializing the$\backslash$r$\backslash$nweights that allows deep autoencoder networks to learn low-dimensional codes that work much$\backslash$r$\backslash$nbetter than principal components analysis as a tool to reduce the dimensionality of data.$\backslash$r$\backslash$n},
annote = {Classic paper on deep learning

Trains multi-layer autoencoder (consisting of encoder and decoder) for the purpose of dimensionality reduction

The autoencoder works on the MNIST dataset, newswire story classification and portrait images

Fast computers, big data sets and good weight initialization were biggest obstacles to deep learning breakthrough
Article shows that all three conditions are now satisfied},
archivePrefix = {arXiv},
arxivId = {20},
author = {Hinton, G. E.},
doi = {10.1126/science.1127647},
eprint = {20},
file = {:Users/felix/code/ml/master-thesis/literature/science.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1127647},
volume = {313},
year = {2006}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
annote = {Review paper explaining deep learning on a basic level

Contains information on basic deep learning architectures such as CNNs, RNNs and LSTM

Shows applications and areas of further research},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:Users/felix/code/ml/master-thesis/literature/NatureDeepReview.pdf:pdf},
isbn = {9780521835688},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:Users/felix/code/ml/master-thesis/literature/ncfast.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
number = {7},
pages = {1527--54},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16764513},
volume = {18},
year = {2006}
}
@article{Kupavskii2012,
abstract = {Retweet cascades play an essential role in information diffusion in Twitter. Popular tweets reflect the current trends in Twitter, while Twitter itself is one of the most important online media. Thus, understanding the reasons why a tweet becomes popular is of great interest for sociologists, marketers and social media researches. What is even more important is the possibility to make a prognosis of a tweet's future popularity. Besides the scientific significance of such possibility, this sort of prediction has lots of practical applications such as breaking news detection, viral marketing etc. In this paper we try to forecast how many retweets a given tweet will gain during a fixed time period. We train an algorithm that predicts the number of retweets during time T since the initial moment. In addition to a standard set of features we utilize several new ones. One of the most important features is the flow of the cascade. Another one is PageRank on the retweet graph, which can be considered as the measure of influence of users. {\textcopyright} 2012 ACM.},
annote = {REFERENCE RESULTS FOR REGRESSION

Build prediction model for number of retweets in a given time period

Using content and text features and gradient boosted decision tree model

Minimizing mean square error for cascade size

New measure of influence: PageRank of a user in the retweet graph

Retweet cascade: graph whose vertices are users that retweeted the tweet
Retweet tree: draw edges for first retweeters in cascade

Possible source of assumption about retweet distribution},
author = {Kupavskii, Andrey and Ostroumova, Liudmila and Umnov, Alexey and Usachev, Svyatoslav and Serdyukov, Pavel and Gusev, Gleb and Kustarev, Andrey},
doi = {10.1145/2396761.2398634},
file = {:Users/felix/code/ml/master-thesis/literature/post1726-kupavskii.pdf:pdf},
isbn = {9781450311564},
journal = {Proceedings of the 21st ACM international conference on Information and knowledge management},
keywords = {Cascade sizes,Commerce,Fixed time,Forecasting,Initial moments,Knowledge management,Marketing,Online media,PageRank,Social media,Social networking (online),Viral marketing,influence,information diffusion,retweet cascade},
number = {OCTOBER},
pages = {2335--2338},
title = {{Prediction of retweet cascade size over time}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84871058841{\&}partnerID=40{\&}md5=eaf796c17b6c884c9f666aaf2fb341e6},
year = {2012}
}
@inproceedings{Zaman2010,
abstract = {We present a new methodology for predicting the spread of information in a so- cial network. We focus on the Twitter network, where information is in the form of 140 character messages called tweets, and information is spread by users for- warding tweets, a practice known as retweeting. Using data of who and what was retweeted, we train a probabilistic collaborative filter model to predict future retweets. We find that the most important features for prediction are the identity of the source of the tweet and retweeter. Our methodology is quite flexible and be used as a basis for other prediction models in social networks.},
annote = {Local prediction model based on collaborative filtering methods

Negative log-score as useful performance evaluation metric},
author = {Zaman, Tauhid R and Herbrich, Ralf and {Van Gael}, Jurgen and Stern, David},
booktitle = {Proceedings of the Computational Social Science and the Wisdom of Crowds Workshop, NIPS},
file = {:Users/felix/code/ml/master-thesis/literature/10.1.1.208.5687.pdf:pdf},
number = {45},
pages = {17599--17601},
title = {{Predicting Information Spreading in Twitter}},
url = {http://research.microsoft.com/pubs/141866/NIPS10{\_}Twitter{\_}final.pdf},
volume = {104},
year = {2010}
}
@article{Peng2011,
abstract = {Among the most popular micro-blogging service, Twitter recently introduced their reblogging service called retweet to allow a user to repopulate another user's content for his followers. It quickly becomes one of the most prominent features on Twitter and an important mean for secondary content promotion. However, it remains unclear what motivates users to retweet and whether the retweeting decisions are predictable based on a user's tweeting history and social relationships. In this paper, we propose modeling the retweet patterns using conditional random fields with a three types of user-tweet features: content influence, network influence and temporal decay factor. We also investigate approaches to partition the social graphs and construct the network relations for retweet prediction. Our experiments demonstrate that CRF can improve prediction effectiveness by incorporating social relationships compared to the baselines that do not.},
annote = {Good definition of prediction problem

Separate three groups of features: user, relationship and tweet features

User and network features more useful for prediction than tweet features},
author = {Peng, Huan-Kai and Zhu, Jiang and Piao, Dongzhen and Yan, Rong and Zhang, Ying},
doi = {10.1109/ICDMW.2011.146},
file = {:Users/felix/code/ml/master-thesis/literature/crf{\_}retweeting{\_}dmcii2011.pdf:pdf},
isbn = {978-0-7695-4409-0},
issn = {2375-9232},
journal = {2011 IEEE 11th International Conference on Data Mining Workshops},
keywords = {Conditional Random Fields,Social Network,Twitter},
pages = {336--343},
title = {{Retweet Modeling Using Conditional Random Fields}},
year = {2011}
}
@misc{Lee2014,
abstract = {There has been much effort on studying how social media sites, such as Twitter, help propagate information in different situations, including spreading alerts and SOS messages in an emergency. However, existing work has not addressed how to actively identify and engage the right strangers at the right time on social media to help effectively propagate intended information within a desired time frame. To address this problem, we have developed two models: (i) a feature-based model that leverages peoples' exhibited social behavior, including the content of their tweets and social interactions, to characterize their willingness and readiness to propagate information on Twitter via the act of retweeting; and (ii) a wait-time model based on a user's previous retweeting wait times to predict her next retweeting time when asked. Based on these two models, we build a recommender system that predicts the likelihood of a stranger to retweet information when asked, within a specific time window, and recommends the top-N qualified strangers to engage with. Our experiments, including live studies in the real world, demonstrate the effectiveness of our work.},
annote = {Try to identify the most effective information propagators (i.e. right people at the right time)

Find strangers on Twitter who will retweet a message when asked

Six groups of features plus intuitive reasoning},
archivePrefix = {arXiv},
arxivId = {1405.3750},
author = {Lee, Kyumin and Mahmud, Jalal and Chen, Jilin and Zhou, Michelle and Nichols, Jeffrey},
booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
doi = {10.1145/2557500.2557502},
eprint = {1405.3750},
file = {:Users/felix/code/ml/master-thesis/literature/1405.3750.pdf:pdf},
isbn = {9781450321846},
pages = {247--256},
title = {{Who Will Retweet This?: Automatically Identifying and Engaging Strangers on Twitter to Spread Information}},
url = {http://doi.acm.org/10.1145/2557500.2557502},
year = {2014}
}
@inproceedings{Kwak2010,
abstract = {Twitter, a microblogging service less than three years old, com- mands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing. We have crawled the entire Twitter site and obtained 41.7 million user profiles, 1.47 billion social relations, 4, 262 trending topics, and 106 million tweets. In its follower-following topology analysis we have found a non-power-lawfollower distribution, a short effec- tive diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks [28]. In order to identify influentials on Twitter, we have ranked users by the number of followers and by PageRank and found two rankings to be sim- ilar. Ranking by retweets differs from the previous two rankings, indicating a gap in influence inferred from the number of followers and that from the popularity of one's tweets. We have analyzed the tweets of top trending topics and reported on their temporal behav- ior and user participation. We have classified the trending topics based on the active period and the tweets and show that the ma- jority (over 85{\%}) of topics are headline news or persistent news in nature. A closer look at retweets reveals that any retweeted tweet is to reach an average of 1, 000 users no matter what the number of followers is of the original tweet. Once retweeted, a tweet gets retweeted almost instantly on next hops, signifying fast diffusion of information after the 1st retweet. To the best of our knowledge this work is the first quantitative study on the entire Twittersphere and information diffusion on it.},
annote = {Possible source of retweet distribution assumptions

Most retweeting in first and second degree network

Evidence of favoritism: people retweet from small number of people

Follower distribution not power-law

Small-world phenomenon in Twitter

Most links are reciprocal, i.e. go both ways

Twitter is more a medium for breaking news than a social network},
archivePrefix = {arXiv},
arxivId = {0809.1869v1},
author = {Kwak, Haewoon and Lee, Changhyun and Park, Hosung and Moon, Sue},
booktitle = {Proceedings of the 19th international conference on world wide web},
doi = {10.1145/1772690.1772751},
eprint = {0809.1869v1},
file = {:Users/felix/code/ml/master-thesis/literature/22-WhatisTwitterASocialNetworkOrANewsMedia.pdf:pdf},
isbn = {9781605587998},
issn = {1932-8036},
keywords = {copyright is held by,degree of,homophily,influential,information diffusion,online social network,pagerank,reciprocity,retweet,separation,the international world wide,twitter,web conference com-},
pages = {591--600},
pmid = {70583450},
title = {{What is Twitter, a Social Network or a News Media?}},
url = {http://dl.acm.org/citation.cfm?id=1772751},
year = {2010}
}
@article{Suh,
author = {Suh, Bongwon and Hong, Lichan and Pirolli, Peter and Chi, Ed H},
file = {:Users/felix/code/ml/master-thesis/literature/2010-04-15-retweetability-v18-final.pdf:pdf},
journal = {2010 IEEE Second International Conference on Social Computing (SocialCom)},
keywords = {-twitter,factor analysis,follower,information diffusion since the,namely the followers of,new set of audiences,original tweet is propagated,retweet,social media,social network,the retweeter,to a,tweet},
pages = {177--184},
title = {{Want to Be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network}},
year = {2010}
}
@article{Zaman2014,
abstract = {We predict the popularity of short messages called tweets created in the micro-blogging site known as Twitter. We measure the popularity of a tweet by the time-series path of its retweets, which is when people forward the tweet to others. We develop a probabilistic model for the evolution of the retweets using a Bayesian approach, and form predictions using only observations on the retweet times and the local network or "graph" structure of the retweeters. We obtain good step ahead forecasts and predictions of the final total number of retweets even when only a small fraction (i.e. less than one tenth) of the retweet paths are observed. This translates to good predictions within a few minutes of a tweet being posted and has potential implications for understanding the spread of broader ideas, memes, or trends in social networks and also revenue models for both individuals who "sell tweets" and for those looking to monetize their reach.},
annote = {Predict popularity of a tweet by predicting the time path of retweets it receives

Comparable to my problem in that they try to predict the eventual number of retweets

Small number of features: timing information, depth of retweet graph and number of followers

Tweets are at least a week old so that there are likely no more retweets occurring

Source for assumption about retweet distribution
Time to reach median of total retweet count: between four minutes and three hours
Most retweets come from the first-degree network
Log-normally distributed reaction times

Positive influence of number of followers
Retweet is less likely the farther away from the root user},
archivePrefix = {arXiv},
arxivId = {1304.6777},
author = {Zaman, Tauhid and Fox, Emily B. and Bradlow, Eric T.},
doi = {10.1214/14-AOAS741},
eprint = {1304.6777},
file = {:Users/felix/code/ml/master-thesis/literature/1304.6777.pdf:pdf},
issn = {19417330},
journal = {Annals of Applied Statistics},
keywords = {Bayesian inference,Forecasting,Social networks,Time series,Twitter},
number = {3},
pages = {1583--1611},
title = {{A bayesian approach for predicting the popularity of tweets}},
volume = {8},
year = {2014}
}
@inproceedings{Petrovic2011,
abstract = {Twitter is a very popular way for people to share informa- tion on a bewildering multitude of topics. Tweets are propa- gated using a variety of channels: by following users or lists, by searching or by retweeting. Of these vectors, retweeting is arguably the most effective, as it can potentially reach the most people, given its viral nature. A key task is predicting if a tweet will be retweeted, and solving this problem fur- thers our understanding of message propagation within large user communities. We carry out a human experiment on the task of deciding whether a tweet will be retweeted which shows that the task is possible, as human performance levels are much above chance. Using a machine learning approach based on the passive-aggressive algorithm, we are able to au- tomatically predict retweets as well as humans. Analyzing the learned model, we find that performance is dominated by so- cial features, but that tweet features add a substantial boost},
annote = {Build prediction model in realistically deployed (i.e. streaming prediction) setting

Predict binary variable, i.e. binary classification task

Compare with human experiment results

Model outperforms humans on the given task

Use passive-aggressive (PA) algorithm

Distinguish between social and tweet features
Use standard social features, but also give valuable intuitions for each feature
Use standard tweet features},
author = {Petrovic, Sasa and Osborne, Miles and Lavrenko, Victor},
booktitle = {Proceedings of the Fifth International Conference on Weblogs and Social Media},
file = {:Users/felix/code/ml/master-thesis/literature/icwsm11.pdf:pdf},
keywords = {poster papers},
pages = {586--589},
title = {{Rt to win! predicting message propagation in twitter}},
url = {http://homepages.inf.ed.ac.uk/miles/papers/icwsm11.pdf{\%}5Cnhttp://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewPDFInterstitial/2754/3209},
year = {2011}
}
